{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whence cometh the L in LU?\n",
    "\n",
    "Last time, we constructed the LU factorization by what may have seemed like a laborious procedure.  Getting $U$ was \"easy\", it was just Gaussian elimination.  But to get $L$, we first wrote out the individual elimination steps as matrices, then inverted them to move them to the other side, then multiplied them together to get $L$.\n",
    "\n",
    "However, it turns out that we can just \"read off\" $L$ much more simply directly from the pivot-row \"multipliers\" that we use during elimination steps.  To see this, let's first write a Julia function to perform Gaussian elimination (without row swaps!) and print out all of the steps, adapting our function from pset 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_gauss (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform Gaussian elimination of A without row swaps, returning U,\n",
    "# while printing a message for each elimination step.\n",
    "function print_gauss(A)\n",
    "    m = size(A,1) # number of rows\n",
    "    U = copy!(similar(A, typeof(inv(A[1,1]))), A)\n",
    "    for j = 1:m   # loop over m columns\n",
    "        for i = j+1:m   # loop over rows below the pivot row j\n",
    "            # subtract a multiple of the pivot row (j)\n",
    "            # from the current row (i) to cancel U[i,j] = Uᵢⱼ:\n",
    "            ℓᵢⱼ = U[i,j]/U[j,j]\n",
    "            println(\"subtracting $ℓᵢⱼ × (row $j) from (row $i)\")\n",
    "            U[i,:] = U[i,:] - U[j,:] * ℓᵢⱼ\n",
    "            U[i,j] = 0 # store exact zero to compensate for roundoff errors\n",
    "        end\n",
    "    end\n",
    "    return U\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it on a randomly chosen $5 \\times 5$ matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       "  4  -2  -7  -4  -8\n",
       "  9  -6  -6  -1  -5\n",
       " -2  -9   3  -5   2\n",
       "  9   7  -9   5  -8\n",
       " -1   6  -3   9   6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [4  -2  -7  -4  -8\n",
    "     9  -6  -6  -1  -5\n",
    "    -2  -9   3  -5   2\n",
    "     9   7  -9   5  -8\n",
    "    -1   6  -3   9   6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtracting 2.25 × (row 1) from (row 2)\n",
      "subtracting -0.5 × (row 1) from (row 3)\n",
      "subtracting 2.25 × (row 1) from (row 4)\n",
      "subtracting -0.25 × (row 1) from (row 5)\n",
      "subtracting 6.666666666666667 × (row 2) from (row 3)\n",
      "subtracting -7.666666666666667 × (row 2) from (row 4)\n",
      "subtracting -3.6666666666666665 × (row 2) from (row 5)\n",
      "subtracting -1.2442748091603053 × (row 3) from (row 4)\n",
      "subtracting -0.4732824427480916 × (row 3) from (row 5)\n",
      "subtracting 33.495145631067295 × (row 4) from (row 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 4.0  -2.0   -7.0    -4.0        -8.0     \n",
       " 0.0  -1.5    9.75    8.0        13.0     \n",
       " 0.0   0.0  -65.5   -60.3333    -88.6667  \n",
       " 0.0   0.0    0.0     0.262087   -0.659033\n",
       " 0.0   0.0    0.0     0.0        31.7767  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_gauss(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, here is the LU factorization of $A$ from the built-in `lu` function, with row-swaps disabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 4.0  -2.0   -7.0    -4.0        -8.0     \n",
       " 0.0  -1.5    9.75    8.0        13.0     \n",
       " 0.0   0.0  -65.5   -60.3333    -88.6667  \n",
       " 0.0   0.0    0.0     0.262087   -0.659033\n",
       " 0.0   0.0    0.0     0.0        31.7767  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U = lu(A, Val{false})\n",
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same $U$ matrix!  Now let's look at $L$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0    0.0       0.0        0.0     0.0\n",
       "  2.25   1.0       0.0        0.0     0.0\n",
       " -0.5    6.66667   1.0        0.0     0.0\n",
       "  2.25  -7.66667  -1.24427    1.0     0.0\n",
       " -0.25  -3.66667  -0.473282  33.4951  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the entries of $L$ below the diagonal are *exactly* the multipliers that were printed out during Gaussian elimination (the factors by which the pivot row is multiplied before it is *subtracted* from a row below it).\n",
    "\n",
    "One way to see this is to consider the matrix product $LU$, which should give $A$.  Consider, for example, the third row of this: the third row of $L$ tells us what linear combinations of the rows of $U$ gives the third row of $A$.  It says:\n",
    "\n",
    "* third row of `A` = `[-2,-9,3,-5,2]` = `-0.5` × (row 1 of U) + 6.66... × (row 2 of U) + (row 3 of U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -2.0\n",
       " -9.0\n",
       "  3.0\n",
       " -5.0\n",
       "  2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[3,1] * U[1,:] + L[3,2] * U[2,:] + U[3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is exactly the *reverse of the elimination steps*, so of course it works.  **Putting the multipliers in $L$ is the right thing!**\n",
    "\n",
    "See section 2.6 of the textbook for more info.\n",
    "\n",
    "Still, computing the $L$ in the LU factorization requires care to put all of the multipliers in the right place with the right sign.  It is a pain for human beings, which is why we typically don't do it when performing Gaussian elimination by hand.  However, computers are great at this kind of tedious bookkeeping, and since keeping track of $L$ requires almost *no extra work*, computers essentially *always* figure out *both* $L$ and $U$ when doing Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LU factorizations\n",
    "\n",
    "Lots of things that you might want to do with a matrix $A$ become easier once you have the $A=LU$ factorization.  Most importantly, it becomes much easier to solve systems of equations.\n",
    "\n",
    "(Exactly *how much* easier is something we'll quantify later.  Short answer: for an $n \\times n$ matrix $A$, it takes around $n^3$ operations to perform Gaussian elimination to get $U$ and $L$, but subsequently solving for $x$ by takes only around $n^2$ operations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Ax=b\n",
    "\n",
    "When we do Gaussian elimination by hand, we convert $Ax = b$ to $Ux = c$ by performing the same elimination steps on $b$ to get $c$ as we performed on $A$ to get $U$.  Often, we do this by \"augmenting\" the matrix $A$ with the right-hand side $b$.  This makes it easier (*for hand calculation*) to keep track of what operations to do on $b$. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " -7\n",
       "  2\n",
       "  4\n",
       " -4\n",
       " -7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = rand(-9:9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "   -7.0    \n",
       "   17.75   \n",
       " -117.833  \n",
       "    1.21628\n",
       "  -40.1748 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, U_and_c = lu([A b], Val{false}) # eliminate augmented matrix (without row swaps)\n",
    "U = UpperTriangular(U_and_c[:, 1:end-1]) # all but last column is U\n",
    "c = U_and_c[:, end] # last column is c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can solve $Ux = c$ by backsubstitution (`U \\ c`), and it should give the same answer (up to roundoff error) as `A \\ b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       "  0.505958   0.505958\n",
       " -0.928506  -0.928506\n",
       "  2.16407    2.16407 \n",
       "  1.46166    1.46166 \n",
       " -1.26428   -1.26428 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[U\\c  A\\b] # print them side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the computer doesn't do this: on a computer, you **almost never augment the matrix with the right-hand-side**.  Instead, you:\n",
    "\n",
    "1. Factor $A = LU$ by Gaussian elimination (not including row swaps, discussed below!), giving $Ax = b \\implies LUx = L(Ux) = b$\n",
    "2. Let $c = Ux$.  Solve $Lc = b$ for $c$ by forward-substitution.\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution.\n",
    "\n",
    "The key point to realize is that solving $Lc = b$ for $c$ involves *exactly the same elimination steps* as if you had augmented the matrix with $b$ during Gaussian elimination.   The bookkeeping is more tedious for a human, but computers are good at bookkeeping, and there turn out to be several practical advantages for computer software to separate solving for $LU$ and solving for $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "   -7.0    \n",
       "   17.75   \n",
       " -117.833  \n",
       "    1.21628\n",
       "  -40.1748 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U = lu(A, Val{false}) # Gaussian elimination without row swaps\n",
    "c = L \\ b # solve Lc = b for c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same $c$ as before!\n",
    "\n",
    "Let's write a little program to write out the steps of forward-substitution so that we can see that they are indeed the elimination steps from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[1] = b[1] = -7.0\n",
      "c[2] = b[2]- 2.25 * c[1] = 17.75\n",
      "c[3] = b[3]- -0.5 * c[1]- 6.666666666666666 * c[2] = -117.83333333333333\n",
      "c[4] = b[4]- 2.25 * c[1]- -7.666666666666666 * c[2]- -1.2442748091603053 * c[3] = 1.2162849872773336\n",
      "c[5] = b[5]- -0.25 * c[1]- -3.6666666666666665 * c[2]- -0.47328244274809156 * c[3]- 33.495145631067324 * c[4] = -40.17475728155195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "   -7.0    \n",
       "   17.75   \n",
       " -117.833  \n",
       "    1.21628\n",
       "  -40.1748 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = similar(b, Float64)\n",
    "for i = 1:length(b)\n",
    "    print(\"c[$i] = b[$i]\")\n",
    "    c[i] = b[i]\n",
    "    for j = 1:i-1\n",
    "        print(\"- $(L[i,j]) * c[$j]\")\n",
    "        c[i] = c[i] - L[i,j] * c[j]\n",
    "    end\n",
    "    println(\" = \", c[i])\n",
    "end\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia, `A \\ b` does this whole process for you implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple right-hand sides and AX = B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we need to solve $Ax=b$ for **multiple right-hand sides** $b_1$, $b_2$, and so on.   Once we have computed $A=LU$ by Gaussian elimination, we can *re-use* $L$ and $U$ to solve each new right-hand side:\n",
    "\n",
    "1. Find $A = LU$ by Gaussian elimination\n",
    "2. Solve $Ax_1 = b_1$ by `x₁ = U \\ (L \\ b₁)`\n",
    "3. Solve $Ax_1 = b_2$ by `x₂ = U \\ (L \\ b₂)`\n",
    "4. etcetera\n",
    "\n",
    "Since solving triangular systems of equations ($L$ or $U$) is easy, this way we only do the hard/expensive part (Gaussian elimination once).\n",
    "\n",
    "Julia provides a shorthand for this process, so you don't have to worry about $L$ and $U$ and explicit forward/backsubstitution.  Instead, you compute `LU = lufact(A)`, which creates an \"LU factorization object\" `LU` that internally stores $L$ and $U$ in a compressed format (along with any permutations/row swaps as discussed below), and then you can do `LU \\ b` for each new right-hand side and it will do the (fast) triangular solves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.LinAlg.LU{Float64,Array{Float64,2}}([4.0 -2.0 … -4.0 -8.0; 2.25 -1.5 … 8.0 13.0; … ; 2.25 -7.66667 … 0.262087 -0.659033; -0.25 -3.66667 … 33.4951 31.7767],[1,2,3,4,5],0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LU = lufact(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       "  0.505958   0.505958\n",
       " -0.928506  -0.928506\n",
       "  2.16407    2.16407 \n",
       "  1.46166    1.46166 \n",
       " -1.26428   -1.26428 "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[LU\\b  A\\b] # print them side by side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, if we let $B = (b_1 \\; b_2 \\; \\cdots)$ be the matrix whose columns are the right-hand sides, and $X = (x_1 \\; x_2 \\; \\cdots)$ be the matrix whose columns are the solutions, then solving $Ax_1 = b_1$, $Ax_2 = b_2$, … is equivalent to solving $AX = B$, because $AX = (Ax_1 \\; Ax_2 \\; \\cdots)$ in the \"matrix × columns\" picture of matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       " -0.87626    -8.72869\n",
       " -0.515124   -6.74427\n",
       "  2.09991     1.8561 \n",
       "  1.6908     12.2282 \n",
       " -2.61717   -11.2915 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b₁ = rand(-9:9, 5)\n",
    "b₂ = rand(-9:9, 5)\n",
    "x₁ = A \\ b₁\n",
    "x₂ = A \\ b₂\n",
    "[x₁ x₂] # print results side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       " -0.87626    -8.72869\n",
       " -0.515124   -6.74427\n",
       "  2.09991     1.8561 \n",
       "  1.6908     12.2282 \n",
       " -2.61717   -11.2915 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = [b₁ b₂]\n",
    "A \\ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives the same answer!  On a computer, solving for a bunch of right-hand sides at once by `A \\ B` is often **more efficient** than solving them one by one (for technical reasons involving the speed of memory access).   Conceptually, it is often convenient to think of many right-hand sides and solutions together, in a matrix, rather than separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row swaps and PA = LU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have mostly ignored the possibility of row swaps.  Row swaps may be *required* if you encounter a zero pivot (assuming there is a nonzero value below it in the same column), but this is often unlikely to occur in practice (especially for random matrices!).\n",
    "\n",
    "However, even as in the example above where no row swaps were *required*, a computer will often do them *anyway*, in order to minimize roundoff errors.  As you saw on pset 1, roundoff errors (the computer only keeps about 15–16 significant digits) can be disastrous if the pivot is merely very small.  So, the computer swaps rows to **make the pivot as big as possible**, as strategy called *partial pivoting*.  As a result, the `lu` function in Julia returns *three* things: $L$, $U$, and the permutation $p$ giving the **re-ordering of the rows** of $A$ that is needed.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0        0.0        0.0       0.0        0.0\n",
       "  1.0        1.0        0.0       0.0        0.0\n",
       "  0.444444   0.0512821  1.0       0.0        0.0\n",
       " -0.111111   0.410256   0.582822  1.0        0.0\n",
       " -0.222222  -0.794872   0.171779  0.0242696  1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, U, p = lu(A)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " 9.0  -6.0  -6.0      -1.0      -5.0     \n",
       " 0.0  13.0  -3.0       6.0      -3.0     \n",
       " 0.0   0.0  -4.17949  -3.86325  -5.62393 \n",
       " 0.0   0.0   0.0       8.67894   9.95297 \n",
       " 0.0   0.0   0.0       0.0      -0.771206"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Int64,1}:\n",
       " 2\n",
       " 4\n",
       " 1\n",
       " 5\n",
       " 3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`p` says tells you in what order we should put the rows of $A$ to match the product $LU$: we should re-order `A` to put row 2 first, then row 4, then row 1, then row 5, then row 3.  We can do this in Julia easily by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       "  9  -6  -6  -1  -5\n",
       "  9   7  -9   5  -8\n",
       "  4  -2  -7  -4  -8\n",
       " -1   6  -3   9   6\n",
       " -2  -9   3  -5   2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[p,:] # A with the rows in order p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should match $LU$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  9.0  -6.0  -6.0  -1.0  -5.0\n",
       "  9.0   7.0  -9.0   5.0  -8.0\n",
       "  4.0  -2.0  -7.0  -4.0  -8.0\n",
       " -1.0   6.0  -3.0   9.0   6.0\n",
       " -2.0  -9.0   3.0  -5.0   2.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L*U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer only stores a list of numbers for `p` because that is the most efficient way to store and work with the permutation.  However, for algebraic manipulations it is often convenient to think of this as a **permutation matrix** $P$ multiplying $A$.  Since $P$ re-orders the *rows* of $A$, it must multiply $A$ on the *left*.  Constructing $P$ is easy: it just has a single 1 in each row indicating what row of $A$ should go there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permutation_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a permutation matrix P from the permutation vector p\n",
    "function permutation_matrix(p)\n",
    "    P = zeros(Int, length(p),length(p))\n",
    "    for i = 1:length(p)\n",
    "        P[i,p[i]] = 1\n",
    "    end\n",
    "    return P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       " 0  1  0  0  0\n",
       " 0  0  0  1  0\n",
       " 1  0  0  0  0\n",
       " 0  0  0  0  1\n",
       " 0  0  1  0  0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = permutation_matrix(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P * A == A[p, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, LU factorization with row swaps corresponds to a factorization\n",
    "\n",
    "$$\n",
    "PA = LU\n",
    "$$\n",
    "\n",
    "Now, to solve $Ax = b$, a more complete process is:\n",
    "\n",
    "1. Factor $PA = LU$\n",
    "2. Multiply $P$ by both sides to give $PAx = LUx = Pb$\n",
    "3. Let $c=Ux$ and solve $Lc = Pb$ for $c$ by forward-substitution\n",
    "3. Solve $Ux = c$ for $x$ by backsubstitution.\n",
    "\n",
    "Of course, Julia does all of this for you automatically with `A \\ b` or `lufact(A) \\ b`, but we can do it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  0.505958\n",
       " -0.928506\n",
       "  2.16407 \n",
       "  1.46166 \n",
       " -1.26428 "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = L \\ b[p] # solve Lc = Pb = b[p]\n",
    "x = U \\ c # solve Ux = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  0.505958\n",
       " -0.928506\n",
       "  2.16407 \n",
       "  1.46166 \n",
       " -1.26428 "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A \\ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, this is the same answer as above!\n",
    "\n",
    "One final point often confuses people here, if you think carefully about the above process.   By writing $PA = LU$, it seems like you *first* decide on the row-reordering of $A$, and *then* compute the LU factorization of $PA$.  But how do you know the proper row-reordering *before* you do elimination?  In fact, this is an illusion: the computer figures out the row-reordering as it goes along (partial pivoting as described above), but it then cleverly works backwards to figure out what reordering it *should* have done in the beginning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular matrices\n",
    "\n",
    "If we encounter a zero pivot (or even just a small pivot, on a computer) during Gaussian elimination, we normally swap rows to bring a nonzero pivot up from a subsequent row.  However, what if there are *no* nonzero values below the pivot in that column?  This is called a [singular matrix](https://en.wikipedia.org/wiki/Invertible_matrix): we can still proceed with Gaussian elimination, but **we can't get rid of the zero pivot**.\n",
    "\n",
    "If you have $Ax=b$ where $A$ is singular, then there will typically (for most right-hand sides $b$) be **no solutions**, but there will occasionally (for very special $b$) be **infinitely many solutions**.  (For $2 \\times 2$ matrices, solving $Ax=b$ corresponds to finding the intersection of two lines, and a singular case corresponds to two parallel lines — either there are no intersections, or they intersect everywhere.)\n",
    "\n",
    "For example, consider the following $4 \\times 4$ matrix $A=LU$:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -1 & 1 &  8 \\\\\n",
    "             6 &  1 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  1 &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "The third pivot in $U$ is zero!   Now, suppose we want to solve $Ax=b$.  We first solve $Lc=b$ to apply the elimination steps to $b$.  This is no problem since $L$ has 1's along the diagonal. Suppose we get $c = (c_1, c_2, c_3, c_4)$.  Then we proceed by backsubstitution to solve $Ux = c$, starting with the last row of $U$:\n",
    "\n",
    "$$\n",
    "1 \\times x_4 = c_4 \\implies x_4 = c_4 \\\\\n",
    "\\color{red}{0 \\times x_3} - 2 \\times x_4 = c_3 \\implies \\mbox{no solution unless } -2 x_4 = -2 c_4 = c_3\n",
    "$$\n",
    "For very special right-hand sides, where $c_3 = 2c_4$, we can plug in *any* $x_3$ and get a solution (infinitely many solutions).  Otherwise, we get *no* solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 2  -1  0   3\n",
       " 4  -1  1   8\n",
       " 6   1  4  15\n",
       " 2  -1  0   0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 0 0 0\n",
    " 2 1 0 0\n",
    " 3 4 1 0\n",
    " 1 0 2 1 ] *\n",
    "[2 -1  0  3\n",
    " 0  1  1  2\n",
    " 0  0  0 -2\n",
    " 0  0  0  1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may think that singular cases are not very interesting.  In reality, **exactly singular square matrices never occur by accident**.  There is always some *deep structure of the underlying problem* that causes the singularity, and understanding this structure is *always* interesting.\n",
    "\n",
    "On the other hand, **nearly singular** matrices (where the pivots are nonzero but very small) *can* occur by accident, and dealing with them is often a delicate problem because they are very sensitive to roundoff errors.  (We call these matrices [ill-conditioned](https://en.wikipedia.org/wiki/Condition_number).)  But that's mostly not a topic for 18.06.\n",
    "\n",
    "Singular **non-square** systems, where you have **more equations than unknowns** are *very* common and important, and lead to *fitting* problems where one *minimizes the error* in the solution.  We will talk more about this later in 18.06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrices are **more singular than others**.  For example, they can have **two zero pivots**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  8 \\\\\n",
    "             6 &  3 & 4 & 15 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  1 \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **three**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             2 & -1 & 0 &  3 \\\\\n",
    "             4 & -2 & 1 &  2 \\\\\n",
    "             6 &  3 & 4 & -2 \\\\\n",
    "             2 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                2 & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "or **four**:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "             0 & -1 & 0 &  3 \\\\\n",
    "             0 & -2 & 1 &  2 \\\\\n",
    "             0 &  3 & 4 & -2 \\\\\n",
    "             0 & -1 & 0 &  0 \\\\\n",
    "            \\end{pmatrix}}_A =\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                1 & 0 & 0 & 0 \\\\\n",
    "                2 & 1 & 0 & 0 \\\\\n",
    "                3 & 4 & 1 & 0 \\\\\n",
    "                1 & 0 & 2 & 1 \\\\\n",
    "            \\end{pmatrix}}_L\n",
    "\\underbrace{\\begin{pmatrix} \n",
    "                \\color{red}{0} & -1 &  0 &  3 \\\\\n",
    "                0 &  \\color{red}{0} &  1 &  2 \\\\\n",
    "                0 &  0 &  \\color{red}{0} & -2 \\\\\n",
    "                0 &  0 &  0 &  \\color{red}{0} \\\\\n",
    "            \\end{pmatrix}}_U\n",
    "$$\n",
    "\n",
    "(Notice how changing only one pivot changes only one column of $A$: each column of $U$ determines one column of $A$ via our \"matrix × columns\" viewpoint on matrix multiplication.)\n",
    "\n",
    "Intuitively, having more zero pivots seems \"more singular\", and requires \"more coincidences\" in the right-hand side to have a solution, and has a \"bigger infinity\" of solutions when there *is* a solution.   We will quantify all of these intuitions later in 18.06, when we begin discussing the [null space](https://en.wikipedia.org/wiki/Kernel_(linear_algebra) and [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra) of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 2  -1  0   3\n",
       " 4  -2  1   8\n",
       " 6  -3  4  15\n",
       " 2  -1  0   0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 0 0 0\n",
    " 2 1 0 0\n",
    " 3 4 1 0\n",
    " 1 0 2 1 ] *\n",
    "[2 -1  0  3\n",
    " 0  0  1  2\n",
    " 0  0  0 -2\n",
    " 0  0  0  1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 2  -1  0   3\n",
       " 4  -2  1   8\n",
       " 6  -3  4  15\n",
       " 2  -1  0  -1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 0 0 0\n",
    " 2 1 0 0\n",
    " 3 4 1 0\n",
    " 1 0 2 1 ] *\n",
    "[2 -1  0  3\n",
    " 0  0  1  2\n",
    " 0  0  0 -2\n",
    " 0  0  0  0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Int64,2}:\n",
       " 0  -1  0   3\n",
       " 0  -2  1   8\n",
       " 0  -3  4  15\n",
       " 0  -1  0  -1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1 0 0 0\n",
    " 2 1 0 0\n",
    " 3 4 1 0\n",
    " 1 0 2 1 ] *\n",
    "[0 -1  0  3\n",
    " 0  0  1  2\n",
    " 0  0  0 -2\n",
    " 0  0  0  0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix inverses\n",
    "\n",
    "It is often conceptually convenient to talk about the *inverse* $A^{-1}$ of a matrix $A$, which exists **for any non-singular square matrix**.   This is the matrix such that $x = A^{-1}b$ solves $Ax = b$ for any $b$.   The inverse is conceptually convenient becuase it allows us to move matrices around in equations *almost* like numbers (except that matrices don't commute!).\n",
    "\n",
    "Another way of defining the inverse of a matrix involves the *identity* matrix $I$.   Here is a $ 5 \\times 5$ identity matrix:\n",
    "\n",
    "$$\n",
    "I = \\begin{pmatrix} 1&0&0&0&0 \\\\ 0&1&0&0&0 \\\\ 0&0&1&0&0 \\\\ 0&0&0&1&0 \\\\ 0&0&0&0&1 \\end{pmatrix}\n",
    "= \\begin{pmatrix} e_1 & e_2 & e_3 & e_4 & e_5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the columns $e_1 \\cdots e_5$ of $I$ are the **unit vectors** in each component.\n",
    "\n",
    "The identity matrix, which can be constructed by `eye(5)` in Julia, has the property that $Ix=x$ for any $x$, and hence $IA = A$ for any (here $5 \\times 5$) matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Int64,2}:\n",
       " 1  0  0  0  0\n",
       " 0  1  0  0  0\n",
       " 0  0  1  0  0\n",
       " 0  0  0  1  0\n",
       " 0  0  0  0  1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ = eye(Int, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ * b == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I₅ * A == A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse matrix $A^{-1}$ is the matrix such that $A^{-1} A = A A^{-1} = I$.\n",
    "\n",
    "Why does this correspond to solving $Ax=b$?  Multiplying both sides on the *left* by $A^{-1}$ (multiplying on the *right* would make no sense: we can't multiply vector×matrix!), we get \n",
    "\n",
    "$$\n",
    "A^{-1}Ax=Ix = x = A^{-1}b\n",
    "$$\n",
    "\n",
    "How do we find $A^{-1}$?  The key is the equation $A A^{-1} = I$, which looks just like $AX=B$ for the **right-hand sides consisting of the columns of the identity matrix**, i.e. the unit vectors.   So, we just solve $Ax=e_i$ for $i=1,\\ldots,5$, or equivalently do `A \\ I` in Julia.  Of course, Julia comes with a built-in function `inv(A)` for computing $A^{-1}$ as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  0.0109991   0.529789  -0.908341  -0.635197  -0.0879927\n",
       "  0.131989    0.35747   -0.900092  -0.622365  -0.055912 \n",
       " -0.235564   -0.179652   0.370302   0.353804  -0.11549  \n",
       " -0.301558   -0.69172    1.48701    1.16499    0.0791323\n",
       "  0.2044      0.678582  -1.29667   -1.05408    0.0314696"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv = A \\ I₅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  2.10942e-15   2.44249e-15  -2.88658e-15  -2.66454e-15   4.996e-16  \n",
       "  1.88738e-15   2.77556e-15  -3.21965e-15  -2.55351e-15   5.27356e-16\n",
       " -8.04912e-16  -9.99201e-16   1.88738e-15   1.11022e-15  -1.94289e-16\n",
       " -3.83027e-15  -4.32987e-15   5.9952e-15    5.55112e-15  -7.63278e-16\n",
       "  3.16414e-15   3.66374e-15  -5.32907e-15  -4.44089e-15   6.73073e-16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv - inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The difference is just roundoff errors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0          -8.32667e-15  -1.49325e-14  -4.88498e-15  -1.0103e-14 \n",
       "  1.46549e-14   1.0          -1.82077e-14  -2.22045e-15  -1.15463e-14\n",
       " -6.02296e-15  -1.77636e-15   1.0          -1.11022e-15   5.55112e-15\n",
       " -1.58068e-14   1.4877e-14    2.36478e-14   1.0           1.4877e-14 \n",
       "  1.38639e-14  -7.68829e-15  -2.05808e-14  -5.77316e-15   1.0        "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ainv * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again, we get $I$ up to roundoff errors because the computer does arithmetic only to 15–16 significant digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       "  1.0           8.88178e-16   0.0           0.0          -5.55112e-17\n",
       "  4.44089e-16   1.0           0.0          -8.88178e-16   0.0        \n",
       "  1.72085e-15  -4.21885e-15   1.0          -4.88498e-15  -1.08247e-15\n",
       " -4.44089e-15   2.66454e-15   5.32907e-15   1.0           7.77156e-16\n",
       " -3.10862e-15   8.88178e-16  -1.77636e-15   5.32907e-15   1.0        "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * Ainv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, $AB \\ne BA$ for two matrices $A$ and $B$.  Why can we multiply $A$ by $A^{-1}$ on either the left or right and get the same answer $I$?  It is fairly easy to see why:\n",
    "\n",
    "$$\n",
    "A A^{-1} = I \\implies A A^{-1} A = IA = A = A (A^{-1} A)\n",
    "$$\n",
    "\n",
    "Since $A (A^{-1} A) = A$, and $A$ is non-singular (so there is a unique solution to this system of equations), we must have $A^{-1} A = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix inverses are funny, however:\n",
    "\n",
    "* Inverse matrices are very convenient in *analytical* manipulations, because they allow you to move matrices from one side to the other of equations easily.\n",
    "\n",
    "* Inverse matrices are **almost never computed** in \"serious\" numerical calculations.  Whenever you see $A^{-1} B$ (or $A^{-1} b$), when you go to *implement* it on a computer you should *read* $A^{-1} B$ as \"solve $AX = B$ by some method.\" e.g. solve it by `A \\ B` or by first computing the LU factorization of $A$ and then using it to solve $AX = B$.\n",
    "\n",
    "One reason that you don't usually compute inverse matrices is that it is wasteful: once you have $PA=LU$, you can solve $AX=B$ directly without bothering to find $A^{-1}$, and computing $A^{-1}$ requires much more work if you only have to solve a few right-hand sides.\n",
    "\n",
    "Another reason is that for many special matrices, there are ways to solve $AX=B$ *much* more quickly than you can find $A^{-1}$.   For example, many large matrices in practice are [sparse](https://en.wikipedia.org/wiki/Sparse_matrix) (mostly zero), and often for sparse matrices you can arrange for $L$ and $U$ to be sparse too.  Sparse matrices are much more efficient to work with than general \"dense\" matrices because you don't have to multiply (or even store) the zeros. Even if $A$ is sparse, however, $A^{-1}$ is usually non-sparse, so you lose the special efficiency of sparsity if you compute the inverse matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
