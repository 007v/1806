{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is our introduction to the concept of [eigenvalues and eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) in 18.06.  Unlike the textbook, however, I'm going to approach the subject a little differently.  I'm going to work *backwards*: starting at what we would *like* to obtain (**make matrices act like scalars**) and go backwards to the methods and conditions to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars are easy, matrices are hard?\n",
    "\n",
    "Multiplying a vector by a scalar is easy to understand.  For example, if we multiply a vector $x$ by 0.1 over and over again, we get\n",
    "\n",
    "$$\n",
    "0.1^n x \\to 0\n",
    "$$\n",
    "\n",
    "The direction remains the same, but the magnitude decreases by a factor of 10 each time we multiply, asympotically going to zero.\n",
    "\n",
    "In contrast, multiplying by a *matrix* is a complicated operation.  Easy for a computer, but hard to *understand* if the matrix is big.  If we multiply a vector $x$ by a matrix $A$ over and over, what happens?\n",
    "\n",
    "$$\n",
    "A^n x \\to ???\n",
    "$$\n",
    "\n",
    "Hard to say at a glance, even if I tell you what $A$ is!\n",
    "\n",
    "Also, lots of things that are easy with scalars $\\lambda$ are hard with matrices $A$. For example:\n",
    "\n",
    "* **Solving** $\\lambda x = b$ is easy: $x = \\lambda^{-1} b$ (unless $\\lambda=0$).  Solving $Ax=b$ is hard; even if $A$ is nonsingular, $A^{-1} b$ is a lot of work to compute.  Inverting matrices is harder than inverting numbers!\n",
    "\n",
    "* It's easy to tell **whether** $\\lambda x = b$ **has a solution**, and whether it is unique: unique solutions if $\\alpha \\ne 0$, and otherwise if $\\lambda=0$ then there are (infinitely many) solutions only for $b=0$.  For $Ax=b$, we need to work out the rank, nullspace, etcetera.\n",
    "\n",
    "* Repeated multiplication (**powers**): $\\lambda^n$ is easy to compute and understand, $A^n$ is hard.\n",
    "\n",
    "  - $\\lambda^n$ will go to zero as $n \\to \\infty$ if $|\\lambda| < 1$, and will blow up if $|\\lambda| > 1$.  What about $A^n$?\n",
    "\n",
    "* Solving the **ordinary differential equation (ODE)** $\\frac{dx}{dt} = \\lambda x$ is easy: $x(t) = e^{\\lambda t} x(0)$.  Solving the *system* of ODEs $\\frac{dx}{dt} = A x$ seems hard.  Maybe $x(t) = e^{A t} x(0)$, but what the heck does $e^A$ even mean?\n",
    "\n",
    "  - The solutions $\\sim e^{\\lambda t}$ will go to zero as $t \\to \\infty$ for real $\\lambda < 0$ and will blow up for $\\lambda > 0$.  What about for $A$?\n",
    "\n",
    "* ...many other tasks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvectors: Where matrices act like scalars\n",
    "\n",
    "Suppose that we could find *some* $x \\ne 0$ for which\n",
    "\n",
    "$$\n",
    "\\boxed{Ax = \\lambda x}\n",
    "$$\n",
    "\n",
    "for some scalar λ.   **For *that* x, the matrix A would act like a scalar λ.**  Multipling, dividing, etcetera by $A$ would be easy for that vector (and multiples thereof)!\n",
    "\n",
    "We call such an $x$ an **eigenvector** of $A$, and $\\lambda$ the corresponding **eigenvalue**.  Of course, $\\alpha x$ for any scalar $\\alpha$ is an eigenvector of the same $\\lambda$, but this is a *subspace*, and we only need to find a *basis* vector $x$.\n",
    "\n",
    "But why should such magical solutions even exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal matrices are almost as easy as scalars\n",
    "\n",
    "If we have an $m \\times m$ [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix) $Λ$, it is almost as easy to work with as a scalar:\n",
    "\n",
    "$$\n",
    "Λ = \\begin{pmatrix} \\lambda_1 & 0 & 0 & 0 \\\\\n",
    "                    0 & \\lambda_2 & 0 & 0 \\\\\n",
    "                    0 & 0 & \\ddots & 0 \\\\\n",
    "                    0 & 0 & 0 & \\lambda_m \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In fact, it is clear that the diagonal elements $\\lambda_k$ are exactly eigenvalues, and the corresponding eigenvectors are the [standard basis vectors](https://en.wikipedia.org/wiki/Standard_basis): the columns of the $m \\times m$ identity matrix $I$.\n",
    "\n",
    "For example, consider the $4\\times4$ matrix with $\\lambda_1 = 0.1$, $\\lambda_2 = 0.2$, $\\lambda_3 = 0.3$, and $\\lambda_4$ = 0.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 0.1  0.0  0.0  0.0\n",
       " 0.0  0.2  0.0  0.0\n",
       " 0.0  0.0  0.4  0.0\n",
       " 0.0  0.0  0.0  0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ = diagm([0.1, 0.2, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has four eigenvectors\n",
    "$$\n",
    "\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n",
    "\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n",
    "\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \n",
    "\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}, \n",
    "$$\n",
    "(along with any multiples of these vectors: again, we only need a basis for the eigenvectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ * [10\n",
    "     0\n",
    "     0\n",
    "     0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.0\n",
       " 2.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ * [0\n",
    "     10\n",
    "     0\n",
    "     0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply a vector $x \\in \\mathbb{R}^4$ by $Λ$, then it multiplies the first component by 0.1, the second by 0.2, etcetera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.1\n",
       " 0.2\n",
       " 0.4\n",
       " 0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,1,1,1]\n",
    "Λ * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying 10 times by $Λ$ just multiplies the first component by $0.1^{10}$, the second by $0.2^{10}$, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 1.0e-10    \n",
       " 1.024e-7   \n",
       " 0.000104858\n",
       " 0.000976563"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ^10 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think about this as writing $x$ in the *basis* of the eigenvectors, and then for *each* eigenvector, the matrix Λ acts like a number λ:\n",
    "\n",
    "$$\n",
    "\\Lambda^n x = \\lambda_1^n \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} +\n",
    "\\lambda_2^n \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} +\n",
    "\\lambda_3^n \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} +\n",
    "\\lambda_4^n \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, the matrix $$ Λ^n = \\begin{pmatrix} \\lambda_1^n & & & \\\\ & \\lambda_2^n & & \\\\ & & \\lambda_3^n & \\\\ & & & \\lambda_4^n \\end{pmatrix}$$ itself must go to zero as $n$ increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 1.0e-100  0.0          0.0          0.0        \n",
       " 0.0       1.26765e-70  0.0          0.0        \n",
       " 0.0       0.0          1.60694e-40  0.0        \n",
       " 0.0       0.0          0.0          7.88861e-31"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ^100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morever, if we multiply $Λ$ by a vector $x$ repeatedly, it will quickly become **nearly parallel** to the last eigenvector $(0,0,0,1)$, since the $0.5^n$ term **decays the most slowly**.  It is expecially easy to see this if we look at the **unit vector** (length = 1)\n",
    "\n",
    "$$\n",
    "\\frac{Λ^n x}{\\Vert Λ^n x \\Vert}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.Slider{Int64}(Signal{Int64}(0, nactions=1),\"\",0,0:100,\"horizontal\",true,\"d\",true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5\n",
       " 0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "comm_id": "6fdd0f28-f257-4474-b15b-b6b2f8caae70",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Interact\n",
    "@manipulate for n = slider(0:100, value=0)\n",
    "    y = Λ^n * x\n",
    "    y / norm(y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagonal matrix is **nonsingular** if the **diagonal entries are nonzero**, and the inverse $Λ^{-1}$ is found by just inverting the diagonal entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 10.0  0.0  0.0  0.0\n",
       "  0.0  5.0  0.0  0.0\n",
       "  0.0  0.0  2.5  0.0\n",
       "  0.0  0.0  0.0  2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(Λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If all matrices were diagonal**, linear algebra would be a lot easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal + Change of Basis is Almost as Easy\n",
    "\n",
    "Suppose we have an invertible $m \\times m$ matrix $X$.   Going from $x$ to $Xc = x$, or $c = X^{-1}x$, is just a **change of basis**: we are writing $x$ in the basis of the **columns of X**, and $y$ are the coefficients.\n",
    "\n",
    "If $X = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_m \\end{pmatrix}$, where the $x_k$ are the columns, then $x=Xc$ is with $c = (c_1,c_2,\\ldots,c_m)$ is equivalent to:\n",
    "\n",
    "$$\n",
    "x = Xc = c_1 x_1 + c_2 x_2 + \\cdots c_m x_m \\; .\n",
    "$$\n",
    "\n",
    "Consider the matrix $A = X Λ X^{-1}$, where $Λ$ is diagonal as above.  What does $Ax$ mean?  If we write:\n",
    "\n",
    "$$\n",
    "Ax = X \\underbrace{ Λ \\underbrace{X^{-1} x}_c }_{Λc}\n",
    "$$\n",
    "\n",
    "we see that $Ax$ is equivalent to:\n",
    "\n",
    "1. Solve $Xc=x$ to express $x$ in the $X$ basis with coefficients $c$.\n",
    "\n",
    "2. Multiply each component of $c$ by the corresponding diagonal entry of $Λ$ to get $Λc$.\n",
    "\n",
    "3. Multiply $Λc$ by $X$ to get $X(Λc)$ in the original ($x$) coordinate system (basis).\n",
    "\n",
    "That is $A$ is just a **diagonal matrix in a different basis**: we change basis, multiply the components by scalars, then change back the basis.\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "Ax = AXc = X Λ c = \\lambda_1 c_1 x_1 + \\lambda_2 c_2 x_2 + \\cdots \\lambda_m c_m x_m \\; .\n",
    "$$\n",
    "\n",
    "Another way of putting this is that the **columns of X are eigenvectors**:\n",
    "\n",
    "$$\n",
    "A x_k = \\lambda_k x_k\n",
    "$$\n",
    "\n",
    "for $k = 1, \\ldots, m$!\n",
    "\n",
    "That is, $A$ just **multiplies each eigenvector $x_k$ by the corresponding eigenvalue $\\lambda_k$**.  Again, **for each eigenvector, the matrix A just acts like a number λ**.\n",
    "\n",
    "For example, with our $4 \\times 4$ matrix $Λ$ from above, let's construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [ 1 0 1  0\n",
    "      2 1 0  1\n",
    "      0 1 1  0\n",
    "      1 0 0 -1]\n",
    "rank(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "  0.325  -0.075   0.075  -0.075\n",
       "  0.025   0.225  -0.025  -0.275\n",
       "  0.15   -0.05    0.25   -0.05 \n",
       " -0.1    -0.1     0.1     0.4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X * Λ / X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't know where it came from, this matrix $A$ wouldn't look like anything special.\n",
    "\n",
    "But multiplying $A$ by **any column of X** just multiplies by a number.  For example, multipling $A$ by the *first column* of X just multiplies by $0.1$ (the first diagonal element of $Λ$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.1\n",
       " 0.2\n",
       " 0.0\n",
       " 0.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * X[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying $A$ by the second column just multiplies it by 0.2, and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.2\n",
       " 0.2\n",
       " 0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * X[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For these special vectors** (the columns of $X$), which we will call **eigenvectors**, the **matrix A acts just like a scalar** (which we call the **eigenvalue**).\n",
    "\n",
    "If we solve $Xc = x$ to write\n",
    "\n",
    "$$\n",
    "x = c_1 x_1 + c_2 x_2 + c_3 x_3 + c_4 x_4\n",
    "$$\n",
    "\n",
    "then $Ax$ is just\n",
    "\n",
    "$$\n",
    "x = 0.1 c_1 x_1 + 0.2 c_2 x_2 + 0.4 c_3 x_3 + 0.5 c_4 x_4\n",
    "$$\n",
    "\n",
    "It is clear, therefore, that multipling *any* vector $x$ repeatedly by $A$ will make it go to zero (the length will shrink exponentially fast).  Let's try our vector $x = (1,1,1,1)$ from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we solve $Xc=x$ to express it in the $X$ basis, then we see that it has components from every column of $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  0.5\n",
       "  0.5\n",
       "  0.5\n",
       " -0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = X\\x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply $A$ times $x$ many times, it must **go to zero** for the same reason as $Λ^n x$ above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       "  8.03469e-41\n",
       " -3.9443e-31 \n",
       "  8.03469e-41\n",
       "  3.9443e-31 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A^100 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this must happen for *any* $x$, $A^n$ itself must go to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "  1.2052e-40   -4.01734e-41   4.01734e-41  -4.01735e-41\n",
       "  1.97215e-31   1.97215e-31  -1.97215e-31  -5.91646e-31\n",
       "  1.2052e-40   -4.01734e-41   4.01734e-41  -4.01735e-41\n",
       " -1.97215e-31  -1.97215e-31   1.97215e-31   5.91646e-31"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A^100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see this is to realize that:\n",
    "\n",
    "$$\n",
    "A^n = \\underbrace{AAA \\cdots  AAA}_{n\\mbox{ times}} = XΛX^{-1}XΛX^{-1}XΛX^{-1}\\cdots XΛX^{-1}XΛX^{-1}XΛX^{-1} = XΛ^n X^{-1}\n",
    "$$\n",
    "\n",
    "which goes to zero because $Λ^n$ goes to zero.\n",
    "\n",
    "The formula means that\n",
    "\n",
    "$$\n",
    "A^n x = XΛ^n \\underbrace{X^{-1}x}_c = \\lambda_1^n c_1 x_1 + \\lambda_2^n c_2 x_2 + \\cdots + \\lambda_m^n c_m x_m\n",
    "$$\n",
    "\n",
    "i.e. **multiplying repeatedly by A just multiples each eigenvector repeatedly by $\\lambda$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Eigenvectors of an arbitrary matrix\n",
    "\n",
    "By constructing $A = X \\Lambda X^{-1}$ above, we got rather arbitrary-looking $m\\times m$ matrices $A$ with $m$ linearly eigenvectors.   This was great, because then the eigenvectors formed a basis of $\\mathbb{R}^m$:  we could **write any vector in the basis of eigenvectors**, and for each eigenvector component the **matrix acts like a scalar**.\n",
    "\n",
    "Can we do the reverse?  For *any* matrix $A$, can we find a basis $X$ of eigenvectors, such that $A = X \\Lambda X^{-1}$ in that basis?  (This is called **diagonalizing** the matrix: finding a basis in which it is diagonal.)  *If* we can find the eigenvectors for an arbitrary $A$, they would tell us a lot — matrices are hard to understand, but scalars are easy, and the eigenvalues tell us what scalars $A$ acts like.\n",
    "\n",
    "It turns out that **this this is almost always possible**.  For *almost* any $m \\times m$ matrix $A$ we (or rather, the computer) can find a basis of exactly $m$ eigenvectors $x_k$ and eigenvalues $\\lambda_k$.  Julia will do this for us, with the `eig(A)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.1,0.2,0.4,0.5],\n",
       "[0.408248 -7.70519e-16 0.707107 2.71948e-16; 0.816497 -0.707107 2.4037e-17 -0.707107; 8.04912e-16 -0.707107 0.707107 1.22377e-15; 0.408248 -4.98571e-16 -3.36518e-16 0.707107])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns exactly the same eigenvalues as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.1\n",
       " 0.2\n",
       " 0.4\n",
       " 0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also returns a matrix whose columns are eigenvectors.  These look a little different than $X$ from above, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       " 0.408248     -7.70519e-16   0.707107      2.71948e-16\n",
       " 0.816497     -0.707107      2.4037e-17   -0.707107   \n",
       " 8.04912e-16  -0.707107      0.707107      1.22377e-15\n",
       " 0.408248     -4.98571e-16  -3.36518e-16   0.707107   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But they are just scalar multiples of the columns of $X$, as we can easily see by multiplying by $X^{-1}$ (writing these eigenvectors in the $X$ basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float64,2}:\n",
       "  0.40825  -0.0       0.0      -0.0    \n",
       "  0.0      -0.70711  -0.0       0.0    \n",
       "  0.0      -0.0       0.70711   0.0    \n",
       " -0.0      -0.0       0.0      -0.70711"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round.(X \\ eigenvectors, 5) # X⁻¹ * eigenvectors, rounded to 5 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **extremely rare exceptions** called [defective matrices](https://en.wikipedia.org/wiki/Defective_matrix) in which you get **too few** eigenvectors to form a basis for the whole $\\mathbb{R}^m$ space. We'll come back to those later, but for now I want to focus on the typical cases of [diagonalizable matrices](https://en.wikipedia.org/wiki/Diagonalizable_matrix) where we *do* have $m$ eigenvectors.\n",
    "\n",
    "Given only $A$, how do we find the eigenvectors and eigenvalues?  How do we know that such solutions exist in the first place, and how many such solutions are there?\n",
    "\n",
    "Let's go back to the beginning.  We are trying to find $x\\ne 0$ satisfying $Ax = \\lambda x$.  Equivalently, we want:\n",
    "\n",
    "$$\n",
    "Ax - \\lambda x = (A - \\lambda I) x = 0\n",
    "$$\n",
    "\n",
    "That is, we want the **nullspace** of $A - \\lambda I$.   For *most* λ, this nullspace will be just $\\{0\\}$.   To get a nontrivial nullspace, we want to\n",
    "\n",
    "* **Find the values of λ such that A-λI is singular.**\n",
    "\n",
    "These λ are the eigenvalues of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The characteristic polynomial\n",
    "\n",
    "Finding the $\\lambda$ such that $A - \\lambda I$ is singular can be theoretically expressed in terms of a concept you will have heard of before: a matrix is singular when its [determinant](https://en.wikipedia.org/wiki/Determinant) is zero.  Hence, we want to solve:\n",
    "\n",
    "$$\n",
    "p(\\lambda) = \\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "This function $p(\\lambda)$ turns out to be a **polynomial of degree m** for an $m\\times m$ matrix $A$, and is called the [characteristic polynomial](https://en.wikipedia.org/wiki/Characteristic_polynomial) of $A$.  Since it is a degree-m polynomial, it will typically have **m roots** (possibly **complex numbers**), the eigenvalues of $A$!\n",
    "\n",
    "To understand this properly, we need to go back and explain determinants.  All you will have seen determinants of 2×2 and 3×3 matrices before (e.g. in 18.02), but you may not have seen them for general $m\\times m$ matrices."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  },
  "widgets": {
   "state": {
    "fe0528ec-263e-4a8e-84a1-b3e545c776b5": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
