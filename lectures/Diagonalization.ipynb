{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonalization\n",
    "\n",
    "If we can find a solution $x \\ne 0$ to\n",
    "\n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "\n",
    "then, for this vector, the matrix $A$ **acts like a scalar**.  $x$ is called an **eigenvector** of $A$, and $\\lambda$ is called an **eigenvalue**.\n",
    "\n",
    "In fact, for an $m \\times m$ matrix $A$, we typically find $m$ linearly independendent eigenvectors $x_1,x_2,\\ldots,x_m$ and $m$ corresponding eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_m$.   Such a matrix is called **diagonalizable**.  Most matrices are diagonalizable; we will deal with the rare \"defective\" (non-diagonalizable) case later.\n",
    "\n",
    "Given such a **basis of eigenvectors**, the key idea for using them is:\n",
    "\n",
    "1. Take any vector $x$ and expand it in this basis: $x = c_1 x_1 + \\cdots c_m x_n$, or $x = Xc$ or $c = X^{-1}x$ where $X$ is the matrix whose *columns are the eigenvectors*.\n",
    "\n",
    "2. For each eigenvector $x_k$, the matrix $A$ acts like a scalar $\\lambda_k$.  Multiplication or division corresponds to multiplying/dividing $x_k$ by $\\lambda_k$.  **Solve your problem for each eigenvector by treating A as the scalar λ**.\n",
    "\n",
    "3. Add up the solution to your problem (sum the basis of the eigenvectors).  That is, multiply the new coefficients by $X$.\n",
    "\n",
    "This process of expanding in the eigenvectors, multiplying (or whatever) by λ, and then summing up the eigenvectors times their new coefficients, is expressed algebraically as the following **diagonalization** of the matrix $A$:\n",
    "\n",
    "$$\n",
    "A = X \\Lambda X^{-1}\n",
    "$$\n",
    "\n",
    "where $\\Lambda$ is the **diagonal matrix of the eigenvalues** and $X = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_m \\end{pmatrix}$ from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding in an Eigenvector Basis\n",
    "\n",
    "For example, consider the matrix\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 1 \\\\ -2 & 4 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "whose eigenvalues are $\\lambda_1 = 2$ and $\\lambda_2 = 3$ and whose eigenvectors are $x_1 = \\begin{pmatrix}1\\\\1\\end{pmatrix}$ and $x_2 = \\begin{pmatrix}1\\\\2\\end{pmatrix}$.\n",
    "\n",
    "We put these eigenvectors into a matrix $X = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$.  The matrix is invertible: $A$ is *diagonalizable*, since the eigenvectors form a *basis*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 2.0\n",
       " 3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1 1\n",
    "    -2 4]\n",
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 1  1\n",
       " 1  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1 1\n",
    "     1 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write any vector $x$ in the basis of eigenvectors, we just want $x = Xc$ or $c = X^{-1} x$.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  2.0\n",
       " -1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,0]\n",
    "c = X \\ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. $x = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, which is obviously correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ax = \\lambda_1 c_1 x_1 + \\lambda_2 c_2 x_2$, or equivalently\n",
    "$$\n",
    "Ax = X \\begin{pmatrix} \\lambda_1 c_1 \\\\ \\lambda_2 c_2 \\end{pmatrix} = \n",
    "     X \\underbrace{\\begin{pmatrix} \\lambda_1 &  \\\\  &  \\lambda_2  \\end{pmatrix}}_\\lambda c\n",
    "     = X \\Lambda X^{-1} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       "  1\n",
       " -2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 2  0\n",
       " 0  3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ = diagm([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       "  1.0\n",
       " -2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X * Λ * inv(X) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is true for *every* $x$, it means $\\boxed{A = X \\Lambda X^{-1}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  1.0  1.0\n",
       " -2.0  4.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X * Λ / X   # / X is equivalent to multiplying by inv(X), but is more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see this is to consider $$AX = \\begin{pmatrix} Ax_1 & Ax_2 \\end{pmatrix} = \\begin{pmatrix} \\lambda_1 x_1 & \\lambda_2 x_2 \\end{pmatrix} = X \\Lambda$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 2  3\n",
       " 2  6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Int64,2}:\n",
       " 2  3\n",
       " 2  6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*Λ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It follows that $A = X\\Lambda X^{-1}$ or $\\boxed{\\Lambda = X^{-1} A X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 2.0  0.0\n",
       " 0.0  3.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X \\ A * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key thing is that this works for **any matrix, as long as the eigenvectors form a basis**. Such a matrix is called **diagonalizable**, and it turns out that this is true for almost all matrices; we will deal with the rare exceptions.\n",
    "\n",
    "Thus, eigenproblems join LU factorization (Gaussian elimination) and QR factorization (Gram–Schmidt): the eigensolutions are **equivalent to a matrix factorization**.  This is extremely useful in helping us think *algebraically* about using eigenvalues and eigenvectors, because it lets us work with them *all at once*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change of Basis and Similar Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant = Product of λ’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace = Sum of λ’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonalization and Matrix Powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
