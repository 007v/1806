{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 11\n",
    "\n",
    "Due **Friday**, May 12 at 11am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "A key fact leading to the SVD is that for any $m\\times n$ real matrix $A$, the positive semidefinite (eigenvalues ≥ 0) matrices $A^T A$ and $A A^T$ have the same nonzero eigenvalues $\\sigma_k^2 > 0$, where the $\\sigma_k$ are called the **singular values** of $A$, for $k = 1,2,\\ldots,r$, for $r=\\operatorname{rank}(A)$.  So, the SVD simultaneously diagonalizes $A^T A$ and $A A^T$.\n",
    "\n",
    "In this problem, you will derive the \"reduced\" form of the SVD based only on what you know about eigenvectors.\n",
    "\n",
    "**(a)** Suppose $\\lambda = \\sigma^2 > 0$ is one of the $r$ nonzero eigenvalues (if any) of $A A^T$ (they cannot be negative because $A A^T$ is positive semidefinite for any $A$).  That is, $AA^T u = \\sigma^2 u$ for some eigenvector $u$, normalized to $u^T u = 1$.   Find an eigenvector $v$ of $A^T A$ with the *same* eigenvalue, normalized to $v^T v = 1$.  (Hint: show that $A^T A (A^T u) = \\cdots$.  Check your $v^T v$ to make sure that it is 1!)\n",
    "\n",
    "**(b)** Why aren't the eigenvectors for λ=0 eigenvalues related in the same way, i.e. why isn't there a 1-to-1 correspondence between the λ=0 eigenvectors of $A A^T$ and $A^T A$, just as in the previous part?  (Hint: long ago, in class, we showed $N(A^T) = N(AA^T)$ for any $A$... this was a key point in least-squares problems.)\n",
    "\n",
    "**(c)** How do your eigenvectors $u$ and $v$ from (a) relate to the solution of problem 2 from pset 9?\n",
    "\n",
    "**(d)** Since $N(A) = N(A^T A)$ (we derived this long ago in class, for least-squares and projection problems), explain why $A x = A \\hat{V} \\hat{V}^T x$ for any $x$, where $\\hat{V}$ is the $n \\times r$ matrix whose columns are the *orthonormal eigenvectors* $v_1,\\ldots,v_r$ of $A^T A$ with positive eigenvalues $\\sigma_1^2,\\ldots,\\sigma_r^2 > 0$.  (Recall that $\\hat{V} \\hat{V}^T$ is the projection operation onto $C(\\hat{V})$.  Hint: $C(\\hat{V})$ is the orthogonal complement of the nullspaces of what matrices?)\n",
    "\n",
    "**(e)** Take the $r = \\operatorname{rank}(A)$ nonzero eigenvectors $\\sigma_k^2$ of $AA^T$ (or $A^T A$) and the corresponding orthonormal eigenvectors $u_k$ and $v_k$ from part (a).  Form the $m \\times r$ matrix $\\hat{U}$ whose columns are $u_1,\\ldots,u_r$, along with the corresponding $\\hat{V}$ matrix from above.  Form the $r \\times r$ *diagonal* matrix $\\hat{\\Sigma}$ whose diagonal entries are $\\sigma_1,\\ldots,\\sigma_r$.\n",
    "\n",
    "* Show that $A\\hat{V} = \\hat{U} \\hat{\\Sigma}$.\n",
    "\n",
    "* Explain why it follows from (d) that $A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T$.  This is the [reduced SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs): in the ordinary SVD you have *square* unitary matrices $U$ and $V$ and a non-square $m \\times n$ diagonal matrix $\\Sigma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Execute the following code cells in the Julia notebook, reading along, and answer the question at the end.\n",
    "\n",
    "The following matrix represents the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set).  Each row is a different flower (150 flowers), and the columns are the measurements (in cm) of the lengths of four different flower parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [5.1 3.5 1.4 0.2; 4.9 3.0 1.4 0.2; 4.7 3.2 1.3 0.2; 4.6 3.1 1.5 0.2; 5.0 3.6 1.4 0.2; 5.4 3.9 1.7 0.4; 4.6 3.4 1.4 0.3; 5.0 3.4 1.5 0.2; 4.4 2.9 1.4 0.2; 4.9 3.1 1.5 0.1; 5.4 3.7 1.5 0.2; 4.8 3.4 1.6 0.2; 4.8 3.0 1.4 0.1; 4.3 3.0 1.1 0.1; 5.8 4.0 1.2 0.2; 5.7 4.4 1.5 0.4; 5.4 3.9 1.3 0.4; 5.1 3.5 1.4 0.3; 5.7 3.8 1.7 0.3; 5.1 3.8 1.5 0.3; 5.4 3.4 1.7 0.2; 5.1 3.7 1.5 0.4; 4.6 3.6 1.0 0.2; 5.1 3.3 1.7 0.5; 4.8 3.4 1.9 0.2; 5.0 3.0 1.6 0.2; 5.0 3.4 1.6 0.4; 5.2 3.5 1.5 0.2; 5.2 3.4 1.4 0.2; 4.7 3.2 1.6 0.2; 4.8 3.1 1.6 0.2; 5.4 3.4 1.5 0.4; 5.2 4.1 1.5 0.1; 5.5 4.2 1.4 0.2; 4.9 3.1 1.5 0.1; 5.0 3.2 1.2 0.2; 5.5 3.5 1.3 0.2; 4.9 3.1 1.5 0.1; 4.4 3.0 1.3 0.2; 5.1 3.4 1.5 0.2; 5.0 3.5 1.3 0.3; 4.5 2.3 1.3 0.3; 4.4 3.2 1.3 0.2; 5.0 3.5 1.6 0.6; 5.1 3.8 1.9 0.4; 4.8 3.0 1.4 0.3; 5.1 3.8 1.6 0.2; 4.6 3.2 1.4 0.2; 5.3 3.7 1.5 0.2; 5.0 3.3 1.4 0.2; 7.0 3.2 4.7 1.4; 6.4 3.2 4.5 1.5; 6.9 3.1 4.9 1.5; 5.5 2.3 4.0 1.3; 6.5 2.8 4.6 1.5; 5.7 2.8 4.5 1.3; 6.3 3.3 4.7 1.6; 4.9 2.4 3.3 1.0; 6.6 2.9 4.6 1.3; 5.2 2.7 3.9 1.4; 5.0 2.0 3.5 1.0; 5.9 3.0 4.2 1.5; 6.0 2.2 4.0 1.0; 6.1 2.9 4.7 1.4; 5.6 2.9 3.6 1.3; 6.7 3.1 4.4 1.4; 5.6 3.0 4.5 1.5; 5.8 2.7 4.1 1.0; 6.2 2.2 4.5 1.5; 5.6 2.5 3.9 1.1; 5.9 3.2 4.8 1.8; 6.1 2.8 4.0 1.3; 6.3 2.5 4.9 1.5; 6.1 2.8 4.7 1.2; 6.4 2.9 4.3 1.3; 6.6 3.0 4.4 1.4; 6.8 2.8 4.8 1.4; 6.7 3.0 5.0 1.7; 6.0 2.9 4.5 1.5; 5.7 2.6 3.5 1.0; 5.5 2.4 3.8 1.1; 5.5 2.4 3.7 1.0; 5.8 2.7 3.9 1.2; 6.0 2.7 5.1 1.6; 5.4 3.0 4.5 1.5; 6.0 3.4 4.5 1.6; 6.7 3.1 4.7 1.5; 6.3 2.3 4.4 1.3; 5.6 3.0 4.1 1.3; 5.5 2.5 4.0 1.3; 5.5 2.6 4.4 1.2; 6.1 3.0 4.6 1.4; 5.8 2.6 4.0 1.2; 5.0 2.3 3.3 1.0; 5.6 2.7 4.2 1.3; 5.7 3.0 4.2 1.2; 5.7 2.9 4.2 1.3; 6.2 2.9 4.3 1.3; 5.1 2.5 3.0 1.1; 5.7 2.8 4.1 1.3; 6.3 3.3 6.0 2.5; 5.8 2.7 5.1 1.9; 7.1 3.0 5.9 2.1; 6.3 2.9 5.6 1.8; 6.5 3.0 5.8 2.2; 7.6 3.0 6.6 2.1; 4.9 2.5 4.5 1.7; 7.3 2.9 6.3 1.8; 6.7 2.5 5.8 1.8; 7.2 3.6 6.1 2.5; 6.5 3.2 5.1 2.0; 6.4 2.7 5.3 1.9; 6.8 3.0 5.5 2.1; 5.7 2.5 5.0 2.0; 5.8 2.8 5.1 2.4; 6.4 3.2 5.3 2.3; 6.5 3.0 5.5 1.8; 7.7 3.8 6.7 2.2; 7.7 2.6 6.9 2.3; 6.0 2.2 5.0 1.5; 6.9 3.2 5.7 2.3; 5.6 2.8 4.9 2.0; 7.7 2.8 6.7 2.0; 6.3 2.7 4.9 1.8; 6.7 3.3 5.7 2.1; 7.2 3.2 6.0 1.8; 6.2 2.8 4.8 1.8; 6.1 3.0 4.9 1.8; 6.4 2.8 5.6 2.1; 7.2 3.0 5.8 1.6; 7.4 2.8 6.1 1.9; 7.9 3.8 6.4 2.0; 6.4 2.8 5.6 2.2; 6.3 2.8 5.1 1.5; 6.1 2.6 5.6 1.4; 7.7 3.0 6.1 2.3; 6.3 3.4 5.6 2.4; 6.4 3.1 5.5 1.8; 6.0 3.0 4.8 1.8; 6.9 3.1 5.4 2.1; 6.7 3.1 5.6 2.4; 6.9 3.1 5.1 2.3; 5.8 2.7 5.1 1.9; 6.8 3.2 5.9 2.3; 6.7 3.3 5.7 2.5; 6.7 3.0 5.2 2.3; 6.3 2.5 5.0 1.9; 6.5 3.0 5.2 2.0; 6.2 3.4 5.4 2.3; 5.9 3.0 5.1 1.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data actually includes 3 different species of flower, and the goal is to figure out how to differentiate between the species based on the above data.  If we number the species 0, 1, and 2 (Iris setosa, Iris virginica and Iris versicolor), then the following array contains the species of each row in $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any data analysis, we should subtract the mean of all the samples (all the flowers), which can be done efficiently in Julia via the \"broadcasting\" `.-` operations combined with the function `mean(X,1)` that returns a row vector of the means of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X .- mean(X,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each flower could be viewed as a point in a four-dimensional space (i.e. the 4 measurements) called [feature space](https://en.wikipedia.org/wiki/Feature_vector).  We'd like to find out what are the important directions in this 4d space in order to explain the variations among the individual flowers, and hopefully to identify the fact that there are three different species.\n",
    "\n",
    "If we just slice this 4d parameter space in a \"random\" direction, e.g. we plot the 4th coordinate (4th column of A) vs the 1st coordinate (1st column of A), we just see a blob of points.  The different colors are the different species, and they are all mixed up together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "scatter(A[:,1], A[:,4], c=species, cmap=\"viridis\")\n",
    "xlabel(\"sepal length (cm)\")\n",
    "ylabel(\"petal width (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform some principal components analysis (PCA) to try to figure out the \"right\" way to look at this data.\n",
    "\n",
    "First, compute the SVD of this matrix $A$, which is equivalent to diagonalizing the covariance matrix $AA^T$ as discussed in class (and in the book):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, σ, V = svd(A)\n",
    "σ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular values σ indicate how much of the variance is explained by each singular vector.\n",
    "\n",
    "Given this information, you should be able to pick out **two directions** in 4d (two directions in feature space, i.e. two combinations of measurements) that **explain most of the variation between**.   \n",
    "\n",
    "Fix the code below to compute two 150-component vectors `x1` and `x2`, corresponding two these two combinations of measurements for each flower, and plot `x2` vs `x1`.\n",
    "\n",
    "You should see that the flowers separate into three groups corresponding to the three different species: PCA identifies the key measurements (or rather, the key combination of measurements) that explain the variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ???\n",
    "x2 = ???\n",
    "scatter(x1, x2, c=species, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out this plot (or print to PDF or make a screenshot and attach it electronically on Stellar) and your ??? code with your solutions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
