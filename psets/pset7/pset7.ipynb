{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 7\n",
    "\n",
    "Due Wednesday, April 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Refer to the [orthogonal polynomials notebook](http://nbviewer.jupyter.org/github/stevengj/1806-spring17/blob/master/lectures/Orthogonal-Polynomials.ipynb) from class for this problem.\n",
    "\n",
    "In class, we defined an dot product $f \\cdot g = \\int_{-1}^1 f(x) g(x) dx$ for functions on $x\\in[-1,1]$, and using this we showed how we could apply Gram–Schmidt to the polynomials $\\{ 1, x, x^2, \\ldots \\}$ to find an orthogonal of polynomials $p_k(x)$, the Legendre polynomials.    \n",
    "\n",
    "### part (a)\n",
    "\n",
    "In class, I claimed that by performing the orthogonal projection of *any* function $f(x)$ onto these polynomials, we obtain the **least-square fit polynomial** on the interval $[-1,1]$.   In this problem, you will apply basic calculus to show that explicitly.\n",
    "\n",
    "Suppose we have an orthonormal basis $q_0(x), q_1(x), q_2(x), q_3(x)$ for all degree ≤ 3 polynomials (the vector space $\\mathcal{P}_3$).  i.e. $q_i \\cdot q_j = 0$ if $i \\ne j$ and $=1$ if $i = j$, using our dot product from above.   Given a real-valued function $f(x)$ on $[-1,1]$ (with finite $f \\cdot f$ — none of these integrals should blow up!), we want to find the *closest* degree-3 polynomial to $f$ in the least-square sense:\n",
    "\n",
    "$$\n",
    "\\min_{p \\in \\mathcal{P}_3} \\int_{-1}^1 [f(x) - p(x)]^2 dx = \\min_{p \\in \\mathcal{P}_3} (f - p) \\cdot (f - p) = \\min_{p \\in \\mathcal{P}_3} \\Vert f - p \\Vert^2\n",
    "$$\n",
    "\n",
    "Write $p(x)$ in our orthonormal basis:\n",
    "\n",
    "$$\n",
    "p(x) = c_0 q_0(x) + c_1 q_1(x) + c_2 q_2(x) + c_3 q_3(x)\n",
    "$$\n",
    "\n",
    "At the minimum $p$ (the least-square fit), basic calculus tells us that the partial derivative must be zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial c_k} \\Vert f - p \\Vert^2 = 0\n",
    "$$\n",
    "\n",
    "**Show that this leads** to the condition $c_k = q_k \\cdot f$, which is exactly the coefficient of the orthogonal projection.\n",
    "\n",
    "Hint: You can easily verify that the product rule $\\frac{\\partial}{\\partial c} (f \\cdot g) = \\left(\\frac{\\partial f}{\\partial c} \\cdot g\\right) + \\left(f \\cdot \\frac{\\partial g}{\\partial c}\\right)$ works for dot products of functions!\n",
    "\n",
    "### part (b)\n",
    "\n",
    "Suppose that we have real-valued function $f(x)$ that is in the span of an infinite orthonormal basis $q_k(x)$ of functions (e.g. polynomials as above) on $[-1,1]$ with the dot product from above, i.e.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=0}^\\infty c_k q_k(x)\n",
    "$$\n",
    "\n",
    "for coefficients $c_k = q_k \\cdot f$.   Assuming $\\Vert f \\Vert$ is finite (i.e. the function $f$ is [square-integrable](https://en.wikipedia.org/wiki/Square-integrable_function)), **derive the identity:**\n",
    "\n",
    "$$\n",
    "\\Vert f \\Vert^2 = f \\cdot f = \\sum_{k=0}^\\infty c_k^2\n",
    "$$\n",
    "\n",
    "(This result is called [Parseval's theorem](https://en.wikipedia.org/wiki/Parseval's_theorem) for Fourier series.) \n",
    "\n",
    "**How does this relate to problem 4 of pset 6?**\n",
    "\n",
    "(For people who have taken 18.100 or similar: assume you can freely interchange/re-order the infinite sums, limits, integrals, etcetera; doing this properly would involve establishing some technical conditions on the infinite series here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Apply Gram-Schmidt to the polynomials ${1, x, x^2}$ to find an orthonormal basis of polynomials under the *different* dot product:\n",
    "\n",
    "$$\n",
    "f \\cdot g = \\int_0^\\infty f(x) g(x) e^{-x} dx\n",
    "$$\n",
    "\n",
    "There are lots of ways to define dot products in practice, and in real applications the choice of dot product depends a lot on the problem you are solving.  For example, one might want to the weight the errors differently at different points (here, weighting by $e^{-x}$) in a least-square fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(Based on Strang, section 6.2, problem 33.)  Consider the following four 2×2 matrices, which have very similar-looking entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = [ 3.  2.\n",
    "      1.  4. ]\n",
    "B = [ 3.  2.\n",
    "     -5. -3. ]\n",
    "C = [ 5.  7.\n",
    "     -3. -4. ]\n",
    "D = [ 5.  6.9\n",
    "     -3. -4.  ]\n",
    "display(A); display(B); display(C); display(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Compute each matrix to the **100th power** in Julia, e.g. compute $A^{100}$ in Julia by `A^100`.  The results should be very different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "All of these matrices are diagonalizable (can be written as $X \\Lambda X^{-1}$ as in lecture), with two distinct eigenvalues $\\lambda$.  The function `eigvals(A)` computes the eigenvalues of $A$ in Julia.  Using the built-in `eigvals` function, compute the eigenvalues of these four matrices, and use them to **explain the results** you observed in part (a).\n",
    "\n",
    "Note that the eigenvalues may be complex numbers, even for real matrices, just as the roots of a real polynomial may be complex!  The complex number $z = a+bi$ in Julia is written `z = a + b*im`.  Complex numbers can also be written in [polar form](http://tutorial.math.lamar.edu/Extras/ComplexPrimer/Forms.aspx) $z = r e^{i\\theta}$, where `r = abs(z)` and `θ = angle(z)` in Julia.  Recall that $z^n = r^n e^{in\\theta}$ [blows up if](http://www.suitcaseofdreams.net/powers_complex.htm) $|z| = r =$ `abs(z)` is $> 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "### (a)\n",
    "\n",
    "Based on Strang, section 5.1, problem 8.   Prove that every orthogonal matrix ($Q^T Q = I$) has determinant $+1$ or $-1$, in two ways:\n",
    "\n",
    "* Use the product rule $\\det (AB) = (\\det A) (\\det B)$ and the transpose rule $\\det Q = \\det Q^T$.\n",
    "* Use only the product rule.  If $|\\det Q|<1$ then $\\det Q^n = (\\det Q)^n$ goes to zero: $Q^n$ becomes nearly singular for large $n$.  How do you know that this can't happen to $Q^n$?\n",
    "  - Hint: $(Q^n)^T (Q^n) = ???$ so $Q^n$ is ???.\n",
    "  - Alternatively, think about problem 4 of pset 6, and note that a nearly singular matrix $A$ has a vector $x\\neq 0$ that is nearly in a nullspace ($Ax$ is nearly zero).\n",
    "\n",
    "### (b)\n",
    "\n",
    "If $\\det A = 1$, does that mean that $A$ is orthogonal?  Explain why or provide a counterexample if it is false.\n",
    "\n",
    "### (c)\n",
    "\n",
    "If $\\det A = 1234$, what is $\\det R$ where $R$ is the upper-triangular matrix in the QR factorization of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "### (a)\n",
    "\n",
    "The function `X = randn(5,5)` in Julia generates a random 5×5 matrix.   Given $X$, we can compute a new matrix $Y = \\alpha X$ for some scalar $\\alpha$ such that $\\det Y = 1234$.  What is $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = randn(5,5)\n",
    "# to make things easier, I'll force det(X) to be positive by flipping the sign of the first column if needed\n",
    "if det(X) < 0\n",
    "    X[:,1] = -X[:,1]\n",
    "end\n",
    "det(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "α = ???    # fill in this line!\n",
    "Y = α * Y\n",
    "det(Y)     # this should give 1234 (+ small roundoff error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Using your matrix $Y$, compute its QR factorization by `Q, R = qr(Y)` and use this to check your answer from problem 4(c) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q, R = qr(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
