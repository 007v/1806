{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 7 - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Refer to the [orthogonal polynomials notebook](http://nbviewer.jupyter.org/github/stevengj/1806-spring17/blob/master/lectures/Orthogonal-Polynomials.ipynb) from class for this problem.\n",
    "\n",
    "In class, we defined an dot product $f \\cdot g = \\int_{-1}^1 f(x) g(x) dx$ for functions on $x\\in[-1,1]$, and using this we showed how we could apply Gram–Schmidt to the polynomials $\\{ 1, x, x^2, \\ldots \\}$ to find an orthogonal of polynomials $p_k(x)$, the Legendre polynomials.    \n",
    "\n",
    "### part (a)\n",
    "\n",
    "In class, I claimed that by performing the orthogonal projection of *any* function $f(x)$ onto these polynomials, we obtain the **least-square fit polynomial** on the interval $[-1,1]$.   In this problem, you will apply basic calculus to show that explicitly.\n",
    "\n",
    "Suppose we have an orthonormal basis $q_0(x), q_1(x), q_2(x), q_3(x)$ for all degree ≤ 3 polynomials (the vector space $\\mathcal{P}_3$).  i.e. $q_i \\cdot q_j = 0$ if $i \\ne j$ and $=1$ if $i = j$, using our dot product from above.   Given a real-valued function $f(x)$ on $[-1,1]$ (with finite $f \\cdot f$ — none of these integrals should blow up!), we want to find the *closest* degree-3 polynomial to $f$ in the least-square sense:\n",
    "\n",
    "$$\n",
    "\\min_{p \\in \\mathcal{P}_3} \\int_{-1}^1 [f(x) - p(x)]^2 dx = \\min_{p \\in \\mathcal{P}_3} (f - p) \\cdot (f - p) = \\min_{p \\in \\mathcal{P}_3} \\Vert f - p \\Vert^2\n",
    "$$\n",
    "\n",
    "Write $p(x)$ in our orthonormal basis:\n",
    "\n",
    "$$\n",
    "p(x) = c_0 q_0(x) + c_1 q_1(x) + c_2 q_2(x) + c_3 q_3(x)\n",
    "$$\n",
    "\n",
    "At the minimum $p$ (the least-square fit), basic calculus tells us that the partial derivative must be zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial c_k} \\Vert f - p \\Vert^2 = 0\n",
    "$$\n",
    "\n",
    "**Show that this leads** to the condition $c_k = q_k \\cdot f$, which is exactly the coefficient of the orthogonal projection.\n",
    "\n",
    "Hint: You can easily verify that the product rule $\\frac{\\partial}{\\partial c} (f \\cdot g) = \\left(\\frac{\\partial f}{\\partial c} \\cdot g\\right) + \\left(f \\cdot \\frac{\\partial g}{\\partial c}\\right)$ works for dot products of functions!\n",
    "\n",
    "### part (b)\n",
    "\n",
    "Suppose that we have real-valued function $f(x)$ that is in the span of an infinite orthonormal basis $q_k(x)$ of functions (e.g. polynomials as above) on $[-1,1]$ with the dot product from above, i.e.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{k=0}^\\infty c_k q_k(x)\n",
    "$$\n",
    "\n",
    "for coefficients $c_k = q_k \\cdot f$.   Assuming $\\Vert f \\Vert$ is finite (i.e. the function $f$ is [square-integrable](https://en.wikipedia.org/wiki/Square-integrable_function)), **derive the identity:**\n",
    "\n",
    "$$\n",
    "\\Vert f \\Vert^2 = f \\cdot f = \\sum_{k=0}^\\infty c_k^2\n",
    "$$\n",
    "\n",
    "(This result is called [Parseval's theorem](https://en.wikipedia.org/wiki/Parseval's_theorem) for Fourier series.) \n",
    "\n",
    "**How does this relate to problem 4 of pset 6?**\n",
    "\n",
    "(For people who have taken 18.100 or similar: assume you can freely interchange/re-order the infinite sums, limits, integrals, etcetera; doing this properly would involve establishing some technical conditions on the infinite series here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "#### part (a)\n",
    "We want to solve\n",
    "$$\\frac{\\partial}{\\partial c_k} \\lVert f-p\\rVert^2=0$$\n",
    "Let us first compute $\\lVert f-p\\rVert^2=(f-p)\\cdot (f-p)$. We can write\n",
    "$$ p = c_1 q_1+c_2q_2+c_3q_3$$\n",
    "Then\n",
    "$$ (f-p)\\cdot (f-p) = \\int_{-1}^1 (f(x)-p(x))^2 dx = \\int_{-1}^1 (f(x)^2 + p(x)^2 - 2f(x)p(x)dx) =$$\n",
    "$$ = \\int_{-1}^2 \\left( f(x)^2 + c_1^2 q_1(x)^2 + c_2^2 q_2(x)^2 + c_3^2 q_3(x)^2 + 2c_1c_2 q_1(x)q_2(x) + 2c_1c_3 q_1(x)q_3(x) + 2 c_2c_3 q_2(x)q_3(x) -2c_1 f(x)q_1(x) -2c_2f(x)q_2(x)-2c_3f(x)q_3(x)\\right)dx=$$\n",
    "$$ = \\lVert f\\rVert^2 + c_1^2 \\lVert q_1\\rVert^2 + c_2^2\\lVert q_2\\rVert^2 + c_3^2\\lVert q_3\\rVert^2 + 2c_1c_2(q_1\\cdot q_2)+ 2c_1c_3(q_1\\cdot q_3)+ 2c_2c_3(q_2\\cdot q_3)-2c_1(f\\cdot q_1)-2c_2(f\\cdot q_2)-2c_3(f\\cdot q_3)=$$\n",
    "$$ = \\lVert f\\rVert^2 +c_1^2+c_2^2+c_3^2-2c_1 (f\\cdot q_1) -2c_2 (f\\cdot q_2)$$\n",
    "Using that $q_i\\cdot q_j$ is 1 if $i=j$ and 0 if $i\\neq j$.\n",
    "\n",
    "Now let us compute $\\frac{\\partial}{\\partial c_1} \\lVert f-p\\rVert^2$.\n",
    "$$\\frac{\\partial}{\\partial c_1} \\lVert f-p\\rVert^2 = 2c_1 - 2 (f\\cdot q_1)$$\n",
    "So the partial derivative is zero if and only if $c_1 = f\\cdot q_1$. The case for $c_2$ and $c_3$ is identical.\n",
    "\n",
    "##### Alternative solution\n",
    "Instead of doing the lengthy computation above we could have noticed that for all $h,g$ we have a **product rule** for the derivative of the dot product:\n",
    "$$\\frac{\\partial}{\\partial c_i} (h\\cdot g) = \\frac{\\partial}{\\partial c_i}\\int_{-1}^1h(x)g(x)dx = \\int_{-1}^1\\left(\\frac{\\partial h}{\\partial c_i}\\right)g(x)+h(x)\\left(\\frac{\\partial p}{\\partial c_i}\\right) dx = \\left(\\frac{\\partial h}{\\partial c_i}\\right)\\cdot g + h\\cdot \\left( \\frac{\\partial g}{\\partial c_i}\\right)$$\n",
    "and so\n",
    "$$\\frac{\\partial}{\\partial c_i} \\lVert f-p\\rVert^2 = \\frac{\\partial(f-p)}{\\partial c_i} \\cdot (f-p) + (f-p) \\cdot \\frac{\\partial(f-p)}{\\partial c_i} = 2 (f-p) \\cdot \\frac{\\partial(f-p)}{\\partial c_i} = -2(f-p) \\cdot q_i = -2(f\\cdot q_i - c_i) = 0\n",
    "$$\n",
    "and hence $c_i = f \\cdot q_i$.  (We used the facts that $\\partial p / \\partial c_i = q_i$ and $p \\cdot q_i = c_i$.)\n",
    "\n",
    "#### Part (b)\n",
    "Writing down the definition\n",
    "$$ \\lVert f\\rVert^2 = f\\cdot f = \\int_{-1}^1 \\left(\\sum_{k=0}^\\infty c_k q_l(x)\\right)\\left(\\sum_{k=0}^\\infty c_k q_k(x)\\right)dx = \\int_{-1}^1 \\left(\\sum_{k,k'=0}^\\infty c_kc_{k'} q_k(x)q_{k'}(x)\\right)dx = \\sum_{k,k'=0}^\\infty c_kc_{k'}\\int_{-1}^1q_k(x)q_{k'}(x)dx = \\sum_{k,k'=0}^\\infty c_k c_{k'} (q_k\\cdot q_{k'})$$\n",
    "Now, since $q_k\\cdot q_{k'}$ is 0 if $k\\neq k'$ and 1 if $k=k'$ we get the only terms contributing to the sum are those for which $k=k'$. That is\n",
    "$$ \\lVert f\\rVert^2 = \\sum_{k=0}^\\infty c_k^2$$\n",
    "as required.\n",
    "\n",
    "This is exactly the same computation as problem 4 of pset 6. This is not a coincidence. In fact, if $x=(c_1,\\dots,c_n)$ then the vector $Qx$ is exactly $\\sum_{k=1}^n c_i q_i$ where $q_i$ are the orthonormal columns of $Q$. So this problem is an infinite-dimensional version of that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Apply Gram-Schmidt to the polynomials ${1, x, x^2}$ to find an orthonormal basis of polynomials under the *different* dot product:\n",
    "\n",
    "$$\n",
    "f \\cdot g = \\int_0^\\infty f(x) g(x) e^{-x} dx\n",
    "$$\n",
    "\n",
    "There are lots of ways to define dot products in practice, and in real applications the choice of dot product depends a lot on the problem you are solving.  For example, one might want to the weight the errors differently at different points (here, weighting by $e^{-x}$) in a least-square fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Note that the polynomials that we get by this procedure are quite famous: they are known as [Laguerre polynomials](https://en.wikipedia.org/wiki/Laguerre_polynomials).  (Like the [Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials) we obtained in lecture, it turns out that there are easier ways to obtain these polynomials, via recurrence relations.)\n",
    "\n",
    "To solve this problem it will be helpful to find the value of the integrals\n",
    "$$I_n=\\int_0^\\infty x^ne^{-x}dx$$\n",
    "For $n=0$ we have, noticing that $e^{-x}$ is the derivative of $-e^{-x}$\n",
    "$$I_0=\\int_0^\\infty e^{-x}dx = -e^{-x}|_0^\\infty = -e^{-\\infty}+e^0=1$$\n",
    "Now to compute $I_n$ we will use integration by parts.\n",
    "$$I_n = \\int_0^\\infty x^n e^{-x}dx = -x^ne^{-x}|_0^\\infty + n\\int_0^\\infty x^{n-1}e^{-x}dx = nI_{n-1}$$\n",
    "So\n",
    "$$\\int_0^\\infty x^n e^{-x} dx = I_n = nI_{n-1}=n(n-1)I_{n-2}=n(n-1)(n-2)I_{n-3}=\\cdots = n! I_0 = n!$$\n",
    "\n",
    "Let us compute the norm of 1\n",
    "$$ \\lVert 1\\rVert^2 = 1\\cdot 1 = \\int_0^\\infty e^{-x}dx = -e^{-x}|^\\infty_0 = -e^{-\\infty}+e^0=1$$\n",
    "So $\\lVert 1 \\rVert = 1$ and there is no need to renormalize.\n",
    "$$q_1=1$$\n",
    "Let us now compute the length of the projection of $x$ onto 1:\n",
    "$$ 1\\cdot x = \\int_0^\\infty xe^{-x} dx = -(1+x)e^{-x}|_0^\\infty = \\left(-\\infty\\cdot e^{-\\infty}+e^0\\right) = 1$$\n",
    "So to make $x$ orthogonal to 1 we need to subtract $1$, that is the orthogonal vector is $x-1$. Let us now compute the length of it\n",
    "$$ \\lVert x-1\\rVert^2 = \\int_0^\\infty (x-1)^2 e^{-x}dx = \\int_0^\\infty e^{-x}dx -2 \\int_0^\\infty xe^{-x}dx + \\int_0^\\infty x^2e^{-x}dx = 1-2+2!=1$$\n",
    "So there is no need to renormalize\n",
    "$$ q_2 = x-1$$\n",
    "\n",
    "Now let us compute the length of the projection of $x^2$ onto $1$ and $x-1$.\n",
    "$$x^2\\cdot 1 = \\int_0^\\infty x^2e^{-x}dx = 2!=2$$\n",
    "$$x^2\\cdot (x-1) = \\int_0^\\infty (x^3-x^2)e^{-x}dx = \\int_0^\\infty x^3e^{-x}dx - \\int_0^\\infty x^2e^{-x}dx=3!-2!=4$$\n",
    "So after removing the projections we obtain\n",
    "$$x^2-2(1) - 4(x-1) = x^2-4x+2$$\n",
    "Finally we just need to renormalize it\n",
    "$$\\lVert x^2-4x+2\\rVert^2 = \\int_0^\\infty (x^2-4x+2)^2e^{-x}dx = \\int_0^\\infty (x^4 +16x^2+4-8x^3+4x^2-16x)e^{-x}dx=\\int_0^\\infty (x^4-8x^3+20x^2-16x+4)e^{-x} = \\int_0^\\infty x^4e^{-x}dx -8\\int_0^\\infty x^3e^{-x}dx +20\\int_0^\\infty x^2e^{-x}dx -16 \\int_0^\\infty xe^{-x}dx +4\\int_0^\\infty e^{-x} = 4!-8\\cdot 3!+20\\cdot 2!-16\\cdot 1!+4\\cdot 0! = 4$$\n",
    "After renormalizing\n",
    "$$q_3 = \\frac{1}{\\sqrt{4}}(x^2-4x+2) = \\frac{1}{2}x^2-2x+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(Based on Strang, section 6.2, problem 33.)  Consider the following four 2×2 matrices, which have very similar-looking entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 3.0  2.0\n",
       " 1.0  4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  3.0   2.0\n",
       " -5.0  -3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  5.0   7.0\n",
       " -3.0  -4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  5.0   6.9\n",
       " -3.0  -4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = [ 3.  2.\n",
    "      1.  4. ]\n",
    "B = [ 3.  2.\n",
    "     -5. -3. ]\n",
    "C = [ 5.  7.\n",
    "     -3. -4. ]\n",
    "D = [ 5.  6.9\n",
    "     -3. -4.  ]\n",
    "display(A); display(B); display(C); display(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Compute each matrix to the **100th power** in Julia, e.g. compute $A^{100}$ in Julia by `A^100`.  The results should be very different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 2.62954e69  5.25907e69\n",
       " 2.62954e69  5.25907e69"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A^100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  1.0  -0.0\n",
       " -0.0   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B^100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " -5.0  -7.0\n",
       "  3.0   4.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C^100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " -1.07139e-7  -1.73747e-7\n",
       "  7.5542e-8    1.19487e-7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D^100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "All of these matrices are diagonalizable (can be written as $X \\Lambda X^{-1}$ as in lecture), with two distinct eigenvalues $\\lambda$.  The function `eigvals(A)` computes the eigenvalues of $A$ in Julia.  Using the built-in `eigvals` function, compute the eigenvalues of these four matrices, and use them to **explain the results** you observed in part (a).\n",
    "\n",
    "Note that the eigenvalues may be complex numbers, even for real matrices, just as the roots of a real polynomial may be complex!  The complex number $z = a+bi$ in Julia is written `z = a + b*im`.  Complex numbers can also be written in [polar form](http://tutorial.math.lamar.edu/Extras/ComplexPrimer/Forms.aspx) $z = r e^{i\\theta}$, where `r = abs(z)` and `θ = angle(z)` in Julia.  Recall that $z^n = r^n e^{in\\theta}$ [blows up if](http://www.suitcaseofdreams.net/powers_complex.htm) $|z| = r =$ `abs(z)` is $> 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 2.0\n",
       " 5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Complex{Float64},1}:\n",
       " 2.42861e-16+1.0im\n",
       " 2.42861e-16-1.0im"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Complex{Float64},1}:\n",
       " 0.5+0.866025im\n",
       " 0.5-0.866025im"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Complex{Float64},1}:\n",
       " 0.5+0.67082im\n",
       " 0.5-0.67082im"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "As we can see, $A^{100}$ has enourmous entries (of the order of $10^{69}$), while $B^{100}=-I$, $C^{100}=-C$ and the entries of $D^{100}$ are tiny (of the order of $10^{-6}$). This can all be explained by their eigenvalues.  As discussed in class, multiplying by a matrix over and over just multiplies the eigenvectors by the eigenvalues λ over and over.  If |λ|>1, it diverges, while if |λ|<1, it decays.  More explicitly, we showed in class that if $E$ is a diagonalizable matrix $E=X\\Lambda X^{-1}$, we can write\n",
    "$$ E^{100} = X\\Lambda X^{-1} X \\Lambda X^{-1}\\cdots X^{-1}X\\Lambda X^{-1} = X\\Lambda^{100}X^{-1}$$\n",
    "So to understand $E^{100}$ it suffices to understand $\\Lambda^{100}$. But $\\Lambda$ is a diagonal matrix, and its diagonal entries are the eigenvalues of $E$. That is, $\\Lambda^{100}$ is a diagonal matrix whose diagonal entries are the 100-th powers of the eigenvalues of $E$.\n",
    "\n",
    "The eigenvalues of A have magnitude bigger than 1, which is why $A^{100}$ is huge.  (The eigenvalues of $A$ are 2 and 5, so the eigenvalues of $A^{100}$ are $2^{100}=1.26\\times 10^{30}$ and $5^{100}=7.89\\times 10^{69}$, so it is not surprising that the entries of $A^{100}=X\\Lambda^{100}X^{-100}$ are of the order of $10^{69}$.)\n",
    "\n",
    "The eigenvalues 0.5±0.67082i of $D$ have magnitude smaller than one.  In Julia, this is the `abs` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366597112327089"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(0.5+0.67082im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.83666\n",
       " 0.83666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs.(eigvals(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $D^{100}$ is exponentially small.  (The eigenvalues of $D$ have absolute value $0.83666<1$. So their 100-th powers of the same order of magnitude as $(0.83666)^{100}=1.7\\times 10^{-8}$. So we would expect the entries of $D^{100}$ to be approximately of the same order of magnitude as that, which is exactly what we see here.)\n",
    "\n",
    "The eigenvalues of $B$ and $C$ are trickier: they are actually of absolute value 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs.(eigvals(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs.(eigvals(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could check this more carefully by solving their characteristic polynomials explicitly. (In fact, we would find that they are all roots of unity: $\\pm i$ for $B$ and sixth roots of unity for $C$.)  So, raising them to the 100-th power does not change their absolute value: they neither grow nor decay, but in fact their real and imaginary parts oscillate in sign.\n",
    "\n",
    "More explicitly, since the eigenvalues of $B$ are $\\pm i$, since $(\\pm i)^4 = 1$ it follows that $B^4 = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  1.0  -0.0\n",
       " -0.0   1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B^4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and hence $B^{100}=(B^4)^25=I^25=I$ as well.\n",
    "\n",
    "The eigenvalues $\\lambda_\\pm = 0.5 \\pm i\\frac{\\sqrt{3}}{2}$ of $C$ have the property that $\\lambda_\\pm^2 = -\\lambda_\\pm$, hence any *even* power of $C$ gives $-C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4999999999999999 + 0.8660254037844386im"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5 + im * √3/2)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Complex{Float64},1}:\n",
       " -0.5+0.866025im\n",
       " -0.5-0.866025im"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(C).^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "### (a)\n",
    "\n",
    "Based on Strang, section 5.1, problem 8.   Prove that every orthogonal matrix ($Q^T Q = I$) has determinant $+1$ or $-1$, in two ways:\n",
    "\n",
    "* Use the product rule $\\det (AB) = (\\det A) (\\det B)$ and the transpose rule $\\det Q = \\det Q^T$.\n",
    "* Use only the product rule.  If $|\\det Q|<1$ then $\\det Q^n = (\\det Q)^n$ goes to zero: $Q^n$ becomes nearly singular for large $n$.  How do you know that this can't happen to $Q^n$?\n",
    "  - Hint: $(Q^n)^T (Q^n) = ???$ so $Q^n$ is ???.\n",
    "  - Alternatively, think about problem 4 of pset 6, and note that a nearly singular matrix $A$ has a vector $x\\neq 0$ that is nearly in a nullspace ($Ax$ is nearly zero).\n",
    "\n",
    "### (b)\n",
    "\n",
    "If $\\det A = 1$, does that mean that $A$ is orthogonal?  Explain why or provide a counterexample if it is false.\n",
    "\n",
    "### (c)\n",
    "\n",
    "If $\\det A = 1234$, what is $\\det R$ where $R$ is the upper-triangular matrix in the QR factorization of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "#### (a)\n",
    "Using the product rule, and remembering $\\det Q = \\det Q^T$\n",
    "$$ 1 = \\det(I) = \\det (Q^TQ) = \\det(Q^T)\\det(Q) = (\\det Q)^2$$\n",
    "So $\\det Q=\\pm 1$\n",
    "\n",
    "Alternatively, we may see that if $Q$ is orthogonal, so is $Q^n$. In fact\n",
    "$$ (Q^n)^T Q^n = Q^T\\cdots Q^T Q \\cdots Q = I$$\n",
    "In particular $Q^n$ cannot be nearly singular In fact, if $Q^n$ were nearly singular, there would be a vector $x$ with $\\lVert x\\rVert \\gg0$ and $\\lVert Q^nx\\rVert$ very close to zero. But by problem 4 of pset 6 $\\lVert Q^nx\\rVert = \\lVert x\\rVert$.\n",
    "#### (b)\n",
    "There are matrices $A$ such that $\\det A=1$ but $A$ is not orthogonal. One example is\n",
    "$$ A=\\begin{pmatrix} 1&1\\\\ 0&1\\end{pmatrix}$$\n",
    "$A$ is upper triangular, so the determinant is the product of the diagonal entries, that is 1. But $A$ is not orthogonal. In fact\n",
    "$$ A^TA = \\begin{pmatrix} 1 &0\\\\1&1\\end{pmatrix}\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix} = \\begin{pmatrix} 1&1\\\\1 &2 \\end{pmatrix}\\neq I$$\n",
    "\n",
    "#### (c)\n",
    "Since $A=QR$ we have\n",
    "$$\\det(A)=\\det(Q)\\det(R) = \\pm1\\det(R)=\\pm\\det(R)$$\n",
    "since we have shown that $\\det(Q)=\\pm1$. Hence $\\det(R)=\\pm\\det(A)=\\pm 1234$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "### (a)\n",
    "\n",
    "The function `X = randn(5,5)` in Julia generates a random 5×5 matrix.   Given $X$, we can compute a new matrix $Y = \\alpha X$ for some scalar $\\alpha$ such that $\\det Y = 1234$.  What is $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.143825593183501"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = randn(5,5)\n",
    "# to make things easier, I'll force det(X) to be positive by flipping the sign of the first column if needed\n",
    "if det(X) < 0\n",
    "    X[:,1] = -X[:,1]\n",
    "end\n",
    "det(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "α = ???    # fill in this line!\n",
    "Y = α * X\n",
    "det(Y)     # this should give 1234 (+ small roundoff error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Using your matrix $Y$, compute its QR factorization by `Q, R = qr(Y)` and use this to check your answer from problem 4(c) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q, R = qr(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution\n",
    "#### (a)\n",
    "We know that the determinant is multilinear in the columns. So when we multiply all columns of $X$ by the same number $α$, the determinant gets multiplied by $α^n$, where $n$ is the number of columns. So, if we want $\\det(αX)=α^5\\det(X)=1234$, we need\n",
    "$$α^5=\\frac{1234}{\\det(X)} \\Rightarrow α = \\sqrt[5]{\\frac{1234}{\\det(X)}}$$\n",
    "Hence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12345.000000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α = (12345/det(X))^(1/5)    # fill in this line!\n",
    "Y = α * X\n",
    "det(Y)     # this should give 1234 (+ small roundoff error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "Let us check that the determinant of $R$ is indeed $\\pm 12345$ (up to a small error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12344.999999999998"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, R = qr(Y)\n",
    "det(R)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
