{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 3 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Suppose that you solve $AX = B$ with\n",
    "$$\n",
    "B = \\begin{pmatrix}\n",
    "1 & 1 & 1 & 1  \\\\\n",
    "0 & 2 & 2 & 2  \\\\\n",
    "0 & 0 & 1 & 1  \\\\\n",
    "0 & 0 & 0 & 1  \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and find that $X$ is\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "1 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 3 & 1 & 0 \\\\\n",
    "2 & 0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### (a)\n",
    "\n",
    "What is $A^{-1}$?\n",
    "\n",
    "(You should not have to apply brute-force Gaussian elimination to invert any matrices, nor should you use Julia in this part. You should be able to show how to do this quickly *by hand*.)\n",
    "\n",
    "(This is not because we care about hand calculation *per se*, but rather because it is useful to be able to recognize and exploit special structure in matrices, and to understand the relationship between solving systems of right-hand-sides and finding $A^{-1}$.)\n",
    "\n",
    "### (b)\n",
    "\n",
    "Evaluate a simple expression to *check* your answer from (a) by brute-force calculation in Julia.\n",
    "\n",
    "For example, you can compute $B^{-1} X^{-1}$ by `inv(B) * inv(X)` in Julia.  There should be some simple product of matrices or matrix inverses that gives $A^{-1}$. Figure it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4x4 Array{Int64,2}:\n",
       " 1  1  0  1\n",
       " 0  0  1  0\n",
       " 1  3  1  0\n",
       " 2  0  0  1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here are the matrices B and X in Julia form\n",
    "B = [1 1 1 1\n",
    "     0 2 2 2\n",
    "     0 0 1 1\n",
    "     0 0 0 1]\n",
    "X = [1 1 0 1\n",
    "     0 0 1 0\n",
    "     1 3 1 0\n",
    "     2 0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inv(B) * inv(X) ## FIX THIS: change to an expression that will give A⁻¹, and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### (a)\n",
    "\n",
    "Since $AX=B$, we have by inverting both sides that $X^{-1}A^{-1}=B^{-1}$. Then, by multiplying on the left by $X$ we finally get\n",
    "\n",
    "$$ A^{-1}=XB^{-1}$$\n",
    "\n",
    "Equivalently, we could write $A=BX^{-1}$ (multiplying on the right by $X^{-1}$) and then invert both sides.   Equivalently, we could multiply both sides of $AX=B$ on the left by $A^{-1}$ to get $X=A^{-1}B$, then moth both sides on the right by $B^{-1}$ to get $XB^{-1} = A^{-1}$.\n",
    "\n",
    "So, to find $A^{-1}$ we must multiply $X$ by the inverse of $B$. Equivalently, we need to solve the system of equations $A^{-1} B = X$ for $A^{-1}$.  One way to proceed is row-by-row: solve $a^T B = x^T$, where $a^T$ is each row of $A^{-1}$ and $x^T$ is the corresponding row of $x$; equivalently, solve $B^T a = x$.  Since $B$ is upper-triangular (hence $B^T$ is lower triangular), this is easy: we could just solve $B^T a = x$ by forward substitution.  For example, to get the *first row* of $A^{-1}$, we could solve\n",
    "$$\n",
    "B^T a = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 1 & 2 & 1 & 0 \\\\ 1 & 2 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = x\n",
    "$$\n",
    "From the first, row we get $a_1 = 1$.  The second row gives $a_1 + 2a_2 = 1$, so $a_2=0$.  The third row gives $a_1 + 2a_2 + a_3 = 0$, so $a_3 = -1$.  The fourth row gives $a_1 + 2a_2 + a_3 + a_4 = 1$, so $a_4 = 1$.  Hence the first row of $A^{-1}$ is $\\begin{pmatrix} 1 & 0 & -1 & 1 \\end{pmatrix}$. We would proceed similarly for the other rows of $A^{-1}$.\n",
    "\n",
    "Alternatively, there is another way to proceed that is probably easier for this particular matrix.  We have $AX=B$, but we really *want* to have $I$ on the right-hand side, since solving $AY=I$ would give $Y=A^{-1}$.  Multiplying both sides on the *right* by $B^{-1}$ gives $Y=XB^{-1}=A^{-1}$ as above, but multiplying on the *right* corresponds to *column* operations, and in particular *the column operations that turn $B$ into $I$*.  If we do the *same* column operations on $X$, that will give $A^{-1}$.   The reason that this viewpoint is useful is that, if we look at $B$, it is easy to see that for *this* $B$ there are very simple column operations that turn it into the identity matrix.  If we just take each column and subtract the preceding column, the matrix becomes *diagonal*, and to make it into $I$ we then just have to divide the second column by 2.  Doing the *same* column operations to $X$ yields:\n",
    "\n",
    "$$A^{-1}=\\begin{pmatrix} 1 & 0 & -1 & 1\\\\ 0 & 0 & 1 & -1\\\\ 1 & 1 & -2 & -1\\\\ 2 & -1 & 0 & 1\\end{pmatrix} $$\n",
    "\n",
    "##### (b)\n",
    "As described above, the expression is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4x4 Array{Float64,2}:\n",
       " 1.0   0.0  -1.0   1.0\n",
       " 0.0   0.0   1.0  -1.0\n",
       " 1.0   1.0  -2.0  -1.0\n",
       " 2.0  -1.0   0.0   1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*inv(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Consider the vector space $\\mathcal{M}$ of $m \\times m$ real-valued matrices for some $m$, say $m=4$.  True or false (and provide a counter-example if *false*).  \n",
    "\n",
    "1. The symmetric matrices in $\\mathcal{M}$ are a subspace (matrices with $A^T = A$).\n",
    "2. The \"skew-symmetric (also called \"antisymmetric\") matrices (those with $A^T = -A$) in $\\mathcal{M}$ are a subspace.\n",
    "3. The invertible matrices in $\\mathcal{M}$ are a subspace.\n",
    "4. The singular matrices in $\\mathcal{M}$ are a subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. True:\n",
    "In fact multiplying a symmetric matrix by a scalar doesn't change the symmetry, and the sum of symmetric matrices is symmetric (all the operations are done to entry separately\n",
    "\n",
    "2. True:\n",
    "For the same reason as before, plus the identities $-(x+y) = -x-y$ and $-(\\lambda x) = \\lambda (-x)$.\n",
    "\n",
    "3. False:\n",
    "The matrices $A=\\begin{pmatrix} 1 & 0 \\\\ 0 & 1\\end{pmatrix}$ and $-A$ are invertible, but $A+(-A)=0$ is not.\n",
    "\n",
    "4. False:\n",
    "The matrices $A=\\begin{pmatrix} 1& 0 \\\\ 0 & 0\\end{pmatrix}$ and $B=\\begin{pmatrix} 0 & 0 \\\\ 0 & 1\\end{pmatrix}$ are singular but their sum is the identity matrix, which is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(Strang, section 3.2, problem 22.) If $AB=0$ then the column space of $B$ is contained in the `______` of $A$.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "If $AB=0$ the it must be true that column space of $B$ is contained in the nullspace of $A$: $C(B) \\subseteq N(A)$.\n",
    "\n",
    "One way to see this is that for any $y \\in C(B)$, we have $y = Bx$ for some $x$.  But then $Ay = ABx = 0x = 0$, so $y \\in N(A)$.\n",
    "\n",
    "Another way to see this is that $AB$ just multiplies $A$ by each column of $B$, so $AB=0$ means that *every column* of $B$ is in $N(A)$, hence $C(B)$ (linear combinations of the columns) must also be in $N(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "(Strang, section 3.2, problem 29.) If $A$ is $4 \\times 4$ and invertible, what is the nullspace of the $4\\times8$ matrix $B = \\begin{pmatrix} A & A \\end{pmatrix}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Write an 8-dimensional vector $v=\\begin{pmatrix} x\\\\ y\\\\\\end{pmatrix}$ where $x$ and $y$ are two 4-dimensional vectors. Then $v$ is in the nullspace of $B$ if and only if $Bv=0$. But\n",
    "$$ Bv = \\begin{pmatrix} A&A\\end{pmatrix}\\begin{pmatrix}x\\\\y\\end{pmatrix} = Ax+Ay= A(x+y)$$\n",
    "So $Bv=0$ if and only if $A(x+y)=0$. Since $A$ is invertible, this can happen if and only if $x+y=0$, that is $x=-y$. So the nullspace of $B$ is composed by those vectors of the form $\\begin{pmatrix} x\\\\-x\\end{pmatrix}$ if $x$ is a 4-dimensional vector.\n",
    "\n",
    "Another way to see this is that if we convert $B$ to rref form, the same row operations that give $I$ in the first 4 columns (since $A$ is invertible the rank is 4) will also give $I$ in the last four columns, so we will get $R = \\begin{pmatrix} I & I \\end{pmatrix}$.  From class, the null space is then simply the column space of\n",
    "$$\\begin{pmatrix} -I \\\\ I \\end{pmatrix}$$\n",
    "i.e. the columns of this matrix are a basis for $N(B)$, and these columns span all vectors of the form $\\begin{pmatrix} x\\\\-x\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "(Strang, section 3.2, problem 23.) The reduced-row echelon form $R$ of a $3\\times3$ matrix with randomly chosen entries is almost sure to be `_________`.   What $R$ is virtually certain if the random matrix is $4 \\times 3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The reduced row echelon form $R$ of a $3\\times 3$ matrix with randomly chosen real entries is almost sure to be the identity — the probability of getting an exact cancellation for one of the pivots is zero. If $R$ is $4\\times 3$, the reduced row echelon form will almost certainly still have 3 pivots, so will be of the form\n",
    "\n",
    "$$\\begin{pmatrix} 1&0&0\\\\ 0 &1& 0\\\\ 0&0&1\\\\ 0&0&0\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "(Strang, section 3.2, problem 58.)  Suppose $R$ is $m \\times n$ of rank $r$, with pivot columns first:\n",
    "\n",
    "$$\n",
    "R = \\begin{pmatrix} I & F \\\\ 0 & 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $I$ is an identity matrix and $0$ denotes a block of zeros.  \n",
    "\n",
    "1. What are the shapes of those four blocks of $R$?\n",
    "2. Find a *right-inverse* matrix $B$ such that $RB=I$ if $r=m$ (the zero blocks are gone).\n",
    "4. What is the reduced-row echelon form of $R^T$?\n",
    "5. What is the reduced-row echelon form of $R^T R$?\n",
    "\n",
    "(In the last four parts, indicate both blocks like $I$ or $0$ and their shapes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "1. $I$ is an $r\\times r$ block, so $F$ has to be an $r\\times (n-r)$ and the two zero blocks are $(m-r)\\times r$ and $(m-r)\\times (n-r)$ respectively.\n",
    "\n",
    "2. It suffices to choose $B=\\begin{pmatrix} I \\\\ 0\\end{pmatrix}$ where the top block is an $r\\times r$ identity matrix and the block of zeros is an $(n-r)\\times r$ block.\n",
    "\n",
    "3. The reduced row echelon form of $R^T$ is\n",
    "$$\\begin{pmatrix} I & 0 \\\\ 0 & 0\\end{pmatrix}$$\n",
    "where the identity is $r\\times r$ and the other blocks are such that the whole matrix is $n\\times m$. This is because\n",
    "$$R^T=\\begin{pmatrix} I & 0\\\\ F^T&0\\end{pmatrix}$$\n",
    "and we can eliminate all the entries in $F^T$ via row operations.\n",
    "\n",
    "4. The reduced row echelon form of $R^TR$ is\n",
    "$$\\begin{pmatrix} I & E \\\\ 0 & D\\end{pmatrix}$$\n",
    "where the identity block is $r\\times r$, $E$ is an $r\\times (n-r)$ block that is identical to $F$ except in the columns above pivots of $D$, where it is 0, the zero block has size $(n-r)\\times r$ and $D$, of size $(n-r)\\times (n-r)$, is the reduced row echelon form of $-F^TF$. In fact, let $C$ be the lower triangular $(n-r)\\times (n-r)$ matrix such thata $-CF^TF=D$ is the row echelon form of $-F^TF$. Then, since\n",
    "$$R^TR=\\begin{pmatrix} I & F\\\\ F^T & 0\\end{pmatrix}$$\n",
    "and we can use row operations to realize the multiplication on the left by the lower triangular matrix\n",
    "$$L=\\begin{pmatrix} I & 0 \\\\ -CF^T& C\\end{pmatrix}$$\n",
    "and\n",
    "$$LR^TR=\\begin{pmatrix} I & F\\\\ 0 & -CF^TF\\end{pmatrix}=\\begin{pmatrix} I & F \\\\ 0 & D\\end{pmatrix}$$\n",
    "Finally we can do row operations to eliminate the columns of $F$ that lie above pivots of $D$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
