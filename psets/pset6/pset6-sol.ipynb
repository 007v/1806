{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 6 - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "(Strang, chapter 4.2, problem 13.)  Suppose $A$ is the 4 by 4 identity matrix with the last column removed, so that $A$ is 4 by 3.  Project $b = (1,2,3,4)$ onto $C(A)$.  What shape is the projection matrix $P$?  What is $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "The matrix $A$ is\n",
    "$$ A = \\begin{pmatrix} 1 & 0 & 0\\\\ 0&1 & 0 \\\\ 0& 0 & 1\\\\ 0& 0 & 0\\end{pmatrix}$$\n",
    "The span of the columns of $A$ is just the 3-dimensional subspace $V=\\{x_4=0\\}$ of those vectors who have the last coordinate equal to 0, so the orthogonal projection onto $V$ is done by making the fourth coordinate equal to zero. That is, the projection of $b$ is $Pb=(1,2,3,0)^T$.\n",
    "\n",
    "Since the columns of $A$ are orthonormal we can compute the projection matrix onto $C(A)$ by\n",
    "$$ P= A A^T = \\begin{pmatrix} 1 & 0 & 0 & 0\\\\ 0& 1&0&0\\\\ 0&0&1&0\\\\ 0&0&0&0\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "(Based on Strang, chapter 4.2, questions 21–23.) If $A$ is full column rank, the projection matrix onto $C(A)$ is $P = A(A^T A)^{-1} A^T$.   Show:\n",
    "\n",
    "* $P^2 = PP = P$.  ($Pb$ is in $C(A)$, so projecting again does not change it.)\n",
    "* $P^T = P$.  That is, show that $P$ is symmetric.  (Remember that $(B^{-1})^T = (B^T)^{-1}$ for any invertible $B$.)\n",
    "* If $A$ is invertible, then show that $P = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "* We can derive $PP=P$ just by direct multiplication:\n",
    "$$P^2= A(A^TA)^{-1}A^TA(A^TA)^{-1}A^T= A(A^TA)^{-1}A^T= P$$\n",
    "But of course, the underlying result should be obvious: once you project onto a subspace, additional projections don't need to do anything to stay in the subspace.  Projecting twice must therefore be the same as projecting once:\n",
    "* Again, we can show $P^T = P$ by direct computation:\n",
    "$$P^T=(A(A^TA)^{-1}A^T)^{-1} = (A^T)^T ((A^TA)^{-1})^T A^T = A ((A^TA)^T)^{-1} A^T = A (A^TA)^{-1} A^T$$\n",
    "where we used the fact that the transpose of the inverse is the inverse of the transpose: $((A^TA)^{-1})^T = ((A^TA)^T)^{-1} = (A^TA)^{-1}$\n",
    "* If $A$ is invertible, there are several ways of seeing that $P=I$.\n",
    "  - If $A$ is invertible then $(A^TA)^{-1}= A^{-1} (A^T)^{-1}$ and so $P=A (A^TA)^{-1} A^T = A A^{-1} (A^T)^{-1} A^T=I \\cdot I = I$\n",
    "  - If $A$ is an invertible $m \\times m$ matrix, then $C(A) = \\mathbb{R}^m$ (the whole space): *every* vector is in $C(A)$ already.  So, projection doesn't need to do anything, and we must have $P = I$.\n",
    "  - $P$ is invertible since it is a product of invertible matrices. But $P^2=P$. So by multiplying by $P^{-1}$ on both sides gives $P = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(Strang, chapter 4.4, problem 4.)  Give an example of:\n",
    "\n",
    "* A matrix $Q$ with orthonormal columns but $QQ^T \\ne I$\n",
    "* Two orthogonal vectors that are not linearly independent.  (Not orthonormal!)\n",
    "* An orthonormal basis for $\\mathbb{R}^3$, including the vector $q_1 = (1,1,1)/\\sqrt{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "* This is generally true for any non-square $Q$, because $\\operatorname{rank}(QQ^T) = \\operatorname{rank}(Q^TQ) = \\operatorname{rank}(I) = n$, where $Q$ is an $m\\times n$ matrix with $m \\ge n$.  That means the $m\\times m$ matrix $QQ^T$ is not full rank unless $m=n$, and therefore it cannot equal $I$.  As a specific example, take $Q$ to be the matrix $A$ of problem 1\n",
    "$$ Q=\\begin{pmatrix} 1&0&0\\\\ 0&1&0 \\\\ 0&0&1\\\\ 0&0&0\\end{pmatrix}$$\n",
    "The columns of $Q$ are orthonormal but as we have seen $QQ^T\\neq I$.\n",
    "* At least one of the vectors simply has to be zero, since that is orthogonal to everything but not linearly independent of anything (since it is any vector times 0).  For example, the vectors $v_1=(1,0)^T$ and $v_2=(0,0)^T$ in $\\mathbb{R}^2$. Clearly $v_1^Tv_2=0$ and so they are orthogonal, but $v_2 = 0 v_1$ so they are not independent.\n",
    "* We just need to start with any set $\\{q_1, v_2, v_3\\}$ of independent vectors that includes $q_1$, and then apply Gram-Schmidt to the other two vectors.  For example:\n",
    "$$ v_2=(1,-1,0),\\ v_3=(0,1,-1) \\; .$$\n",
    "To save ourselves work, we conveniently chose these vectors to already be orthogonal to $q_1$.  So, on the first Gram–Schmidt step we simply need to normalize $v_2$ to obtain $q_2$:\n",
    "$$ q_2 = \\frac{v_2}{\\sqrt{v_2^Tv_2}} = (1/\\sqrt{2},-1/\\sqrt{2},0)^T \\; .$$\n",
    "On the second Gram–Schmidt step, we have to remove the $q_1$ and $q_2$ components from $v_2$, but since $q_1^T v_2 = 0$ already we just need to remove the $q_2$ component:\n",
    "$$ w_3 = v_3 - q_2 q_2^Tv_3  = (0,1,-1) - (-1/2,1/2,0) = (1/2,1/2,-1)$$\n",
    "Finally we normalize this to a unit vector:\n",
    "$$q_3 = \\frac{w_3}{\\sqrt{w_3^Tw_3}} = (1/\\sqrt{6},1/\\sqrt{6},-2/\\sqrt{6})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "If $Q$ is an $m \\times n$ matrix with orthonormal columns then $(Qx)^T (Qy) = $ ???? for any $x$ and $y$ in $\\mathbb{R}^n$, and hence $\\Vert Qx \\Vert$ = ???? for any $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "We have\n",
    "$$(Qx)^T(Qy) = x^TQ^TQy = x^Ty$$\n",
    "and in particular\n",
    "$$ \\Vert Qx\\Vert= \\sqrt{(Qx)^TQx} = \\sqrt{x^Tx} = \\Vert x\\Vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "The standard (\"classical\") Gram–Schmidt algorithm is notoriously susceptible to roundoff errors when vectors are nearly parallel.  (There are much better algorithms that are not covered in 18.06.)  In this problem, you will explore that susceptibility.\n",
    "\n",
    "Apply the Gram–Schmidt algorithm to find an orthonormal basis for the column space of\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 1 & 1 \\\\\n",
    "                    \\epsilon & 0 & 0 \\\\\n",
    "                    0 & \\epsilon & 0 \\\\\n",
    "                    0 & 0 & \\epsilon\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "where $\\epsilon = 10^{-10}$.\n",
    "\n",
    "However, *round* your calculations (as you do each operation!) to about 15 significant digits.  For example, approximate $1 + \\epsilon^2 \\approx 1$.   Are your resulting basis vectors close to orthogonal?\n",
    "\n",
    "In comparison, Julia's `qr` function uses a much more accurate method (\"Householder reflectors\").  Try it and see how close its vectors are to the ones you came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " -1.0       7.07107e-11   4.08248e-11\n",
       " -1.0e-10  -0.707107     -0.408248   \n",
       "  0.0       0.707107     -0.408248   \n",
       "  0.0       0.0           0.816497   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ϵ = 1e-10\n",
    "A = [1 1 1\n",
    "     ϵ 0 0\n",
    "     0 ϵ 0\n",
    "     0 0 ϵ]\n",
    "Q = qr(A)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how close Julia's $Q$ is to orthonormal columns by computing $Q^T Q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 1.0   0.0           0.0        \n",
       " 0.0   1.0          -1.66533e-16\n",
       " 0.0  -1.66533e-16   1.0        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q'*Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **very close to the identity**, up to the usual 16th-digit roundoff errors, so the `qr` function is clearly doing a good job... much better than what you should get by applying Gram–Schmidt below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "Let $v_1,v_2,v_3$ be the columns of $A$. First we renormalize the first column. The length of the first column is\n",
    "$$ \\sqrt{v_1^Tv_1}=\\sqrt{1+\\epsilon^2}\\approx 1$$\n",
    "(since $1+\\epsilon^2\\approx 1$). So $$\\boxed{q_1 = (1,\\epsilon,0,0)}$$ is just the first column. Now let us subtract the projection of $v_2$ onto $q_1$:\n",
    "$$ w_2 =v_2 - q_2 (q_2^T v_2) = (1,0,\\epsilon,0) - (1,\\epsilon,0,0) \\times 1 = (0,-\\epsilon,\\epsilon,0)$$\n",
    "The length of $w_2$ is\n",
    "$\\sqrt{w_2^Tw_2} = \\sqrt{2\\epsilon^2} = \\sqrt{2}\\epsilon$\n",
    "so after normalization we get\n",
    "$$ \\boxed{q_2 = \\frac{w_2}{\\sqrt{2}\\epsilon} = (0,-1,1,0)/\\sqrt{2}}$$\n",
    "Let now remove from $v_3$ the projections onto $q_1$ and $q_2$.  Since $q_2^T v_3 = 0$, we just need to project out $q_1$:\n",
    "$$ w_3 = v_3 - q_1(q_1^T v_3) - q_2(q_2^T v_3) = (1,0,0,\\epsilon) -  (1,\\epsilon,0,0)\\times 1 - q2 \\times 0 = (0,-\\epsilon,0,\\epsilon)$$\n",
    "Its length is again\n",
    "$\\Vert w_3 \\Vert = \\sqrt{2}\\epsilon$\n",
    "so we get the unit vector\n",
    "$$ \\boxed{q_3 = \\frac{w_3}{\\Vert w_3\\Vert} = (0,-1,0,1)/\\sqrt{2}}$$\n",
    "That is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Array{Float64,2}:\n",
       " 1.0       0.0        0.0     \n",
       " 1.0e-10  -0.707107  -0.707107\n",
       " 0.0       0.707107   0.0     \n",
       " 0.0       0.0        0.707107"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q̃ = [ 1    0     0\n",
    "      ϵ  -1/√2 -1/√2\n",
    "      0   1/√2   0\n",
    "      0    0    1/√2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       "  1.0          -7.07107e-11  -7.07107e-11\n",
       " -7.07107e-11   1.0           0.5        \n",
       " -7.07107e-11   0.5           1.0        "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q̃'*Q̃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **very different** from the expected $I$ matrix.  The problem is that $q_2^T q_3 = 1/2$, which is **very far from orthogonal**.\n",
    "\n",
    "This is the basic problem with Gram–Schmidt, especially the \"classical\" Gram–Schmidt process as presented in class and in the textbook: if the starting vectors are close to being dependent (e.g. nearly parallel), then in the presence of roundoff errors it can produce very non-orthogonal results.   There is a \"modified\" Gram–Schmidt process presented in the textbook that is much better, but in practice computers nowadays use an algorithm that is better still, called \"Householder QR\" — this is why the Julia `qr(A)` function did such a good job, above.\n",
    "\n",
    "Gram–Schmidt is still quite a useful way to *think* about orthogonalization and QR factorization.  But a lot of care is required in translating *conceptual* algorithms into *practical* algorithms.  (Practical algorithms are not the focus of 18.06, but are covered in 18.335 and many textbooks, such as *Numerical Linear Algebra* by Trefethen and Bau.  But is important to be at least vaguely aware of the difference between our conceptual approach and a serious numerical implementation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "In class, we showed that applying Gram–Schmidt to the columns of $A$, **from left to right**, corresponds to a factorization $A=QR$ where $R$ is upper-triangular and the columns of $Q$ are our orthonormal basis ($Q^T Q = I$).\n",
    "\n",
    "If you do Gram–Schmidt from **right to left** on the columns of $A$, what would the corresponding factorization of the matrix $A$ look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Suppose we do Gram–Schmidt from right-to left on the columns of $A = \\begin{pmatrix}a_1 & a_3 & \\cdots & a_n \\end{pmatrix} $.   That is:\n",
    "\n",
    "$$\n",
    "\\tilde{q}_n = \\frac{a_n}{\\Vert a_n \\Vert} \\\\\n",
    "\\tilde{q}_{n-1} = \\frac{a_{n-1} - \\tilde{q}_n \\tilde{q}_n^T a_{n-1}}{\\Vert \\cdots \\Vert} \\\\\n",
    "\\vdots\n",
    "$$\n",
    "\n",
    "We clearly still get a matrix $\\tilde{Q} = \\begin{pmatrix}\\tilde{q}_1 & \\tilde{q}_2 & \\cdots & \\tilde{q}_n \\end{pmatrix} $ with orthonormal columns made from the columns of $A$.   Furthermore since these are **column** operations, they corresponding to multiplying $A$ by another matrix **on the right** to get $\\tilde{Q}$ or vice-versa.  So, we still must have a factorization of the form:\n",
    "\n",
    "$$\n",
    "A = \\tilde{Q} \\tilde{R}\n",
    "$$\n",
    "\n",
    "What does the matrix $\\tilde{R}$ look like?  The **k-th column** of $\\tilde{R}$ indicates what **linear combination** of the columns of $\\tilde{Q}$ gives $a_k$ (the k-th column of $A$).   Since we did Gram–Schmidt from right to left, by construction we know that $a_k$ is in the span of $\\{ \\tilde{q}_k, \\tilde{q}_{k+1}, \\ldots, \\tilde{q}_n \\}$, i.e. $a_k$ is made from **columns k to n of Q̃**.  That means that the k-th column of $\\tilde{R}$ only has **nonzero entries in rows ≥ k**.  But this is precisely a **lower-triangular matrix**.\n",
    "\n",
    "So, Gram–Schmidt from right to left on an $m\\times n$ matrix $A$ (assumed full column rank, as in class) produces the factorization $A = \\tilde{Q} \\tilde{R}$ where the $m\\times n$ matrix $\\tilde{Q}$ has orthonormal columns ($\\tilde{Q}^T \\tilde{Q} = I$) and the $n\\times n$ matrix $\\tilde{R}$ is an invertible lower-triangular matrix.\n",
    "\n",
    "#### Alternate solution\n",
    "\n",
    "Let\n",
    "$$P=\\begin{pmatrix} 0 & 0 & \\cdots & 0 & 1\\\\ 0& 0 & \\cdots & 1 & 0\\\\ & \\vdots &\\\\1 & 0 & \\cdots & 0 & 0\\end{pmatrix}$$\n",
    "be the permutation matrix corresponding to the permutation that reverses the order of the components. Then applying Gram-Schmidt from right to left is the same thing as applying Gram-Schmidt to $AP$ ($A$ with columns in reverse order). That is let\n",
    "$$AP=Q'R'$$\n",
    "be the classical QR factorization of $AP$. Then the matrix $Q$ that we obtain by doing Gram-Schmidt right to left is $Q'P$ ($Q'$ with columns reversed to match $A$) and so the corresponding factorization of $A$ is\n",
    "$$ A = (AP)P = Q'R'P = (Q'P)(PR'P)$$\n",
    "\n",
    "Comparing to above, this is $A=\\tilde{Q} \\tilde{R}$, where $\\tilde{Q} = Q'P$ and $\\tilde{R} = PR'P$.   The $\\tilde{Q}$ matrix still has orthonormal columns, of course (explicitly: $\\tilde{Q}^T \\tilde{Q} = P^T Q'^T Q' P = P^T P = I$).  The matrix $\\tilde{R} = PR'P$ is lower triangular, since $R'$ is upper triangular and $PR'P$ flips both its rows and columns as in problem 4a of pset 2."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
